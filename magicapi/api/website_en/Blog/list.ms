{
  "properties" : { },
  "id" : "81b4dfd2ac344ee3855104d3a7c4a589",
  "script" : null,
  "groupId" : "43ae7ed0e6b94cc085570668c6c90523",
  "name" : "list",
  "createTime" : null,
  "updateTime" : 1669110880824,
  "lock" : null,
  "createBy" : null,
  "updateBy" : "admin",
  "path" : "/list",
  "method" : "GET",
  "parameters" : [ {
    "name" : "page",
    "value" : "1",
    "description" : null,
    "required" : false,
    "dataType" : "String",
    "type" : null,
    "defaultValue" : null,
    "validateType" : null,
    "error" : null,
    "expression" : null,
    "children" : null
  }, {
    "name" : "pageSize",
    "value" : "10",
    "description" : null,
    "required" : false,
    "dataType" : "String",
    "type" : null,
    "defaultValue" : null,
    "validateType" : null,
    "error" : null,
    "expression" : null,
    "children" : null
  }, {
    "name" : "author",
    "value" : "NebulaGraph",
    "description" : null,
    "required" : false,
    "dataType" : "String",
    "type" : null,
    "defaultValue" : null,
    "validateType" : null,
    "error" : null,
    "expression" : null,
    "children" : null
  } ],
  "options" : [ ],
  "requestBody" : "",
  "headers" : [ ],
  "paths" : [ ],
  "responseBody" : "{\n    \"code\": 0,\n    \"message\": \"success\",\n    \"data\": {\n        \"total\": 130,\n        \"list\": [{\n            \"id\": 309,\n            \"createTime\": \"2022-08-26T04:49:30\",\n            \"updateTime\": \"2022-08-26T04:49:30\",\n            \"author\": \"NebulaGraph\",\n            \"content\": \"This is a generated dataset with two kinds of vertices and four kinds of edges(relationships):\\n\\nperson can hold a corp in {share} % person can be relative with another person corp can hold another corp in {share} % corp can be a branch of another corp person can be as a role of a corp.\\n\\n\\n![data-modeling-for-graph](https://www-cdn.nebula-graph.io/nebula-website-5.0/images/demo/demo-shareholding/graph-modeling.png)\\n\\n\\nWithin the playground, you can visually explore the shareholding data from select vertices(i.e. “c_132” with the name “Chambers LLC”) by selecting:\\n\\n![data-import](https://www-cdn.nebula-graph.io/nebula-website-5.0/images/demo/demo-shareholding/graph-database-vertices.png)\\n\\n\\nclick this explored vertex dot, then you can explore from select vertices by selecting:\\n\\n* Edge Type\\n* Direction\\n* Steps\\n* Query Limit(Optional)\\n\\nnote, you can click the \\uD83D\\uDC41️ icon to add options to show fields of the graph,\\n\\n![graph-database-canvas](https://www-cdn.nebula-graph.io/nebula-website-5.0/images/demo/demo-shareholding/graph-database-canvas.png)\\n\\n\\nAfter clicking Expand, you will see all queried relations with c_132 the Chambers LLC.\\n\\n![graph-database-visualization](https://www-cdn.nebula-graph.io/nebula-website-5.0/images/demo/demo-shareholding/graph-database-visualization.png)\\n\\nAlternatively, you could query nGQL via console like:\\n\\nGO 1 TO 3 STEPS FROM \\\"c_132\\\" over * BIDIRECT;\\n\\nRead for more of the dataset, refer to [https://github.com/wey-gu/nebula-shareholding-example](https://github.com/wey-gu/nebula-shareholding-example])\\n\\nDownload sample dataset: [shareholding-dataset](https://www.kaggle.com/littlewey/nebula-graph-shareolding-dataset)\\n\\n\",\n            \"description\": \"NebulaGraph Dashboard is an out-of-the-box visualized cluster management tool. It enables cluster operations such as node management, service management, scaling in/out, and configuration update.\",\n            \"pic\": \"https://www-cdn.nebula-graph.io/nebula-website-5.0/images/demo/demo-shareholding/shareholding-banner.png\",\n            \"publish\": true,\n            \"slug\": \"shared-holding\",\n            \"title\": \"Use NebulaGraph to explore shareholding networks\",\n            \"createUserId\": 8,\n            \"updateUserId\": 8,\n            \"blogCategoryId\": 2,\n            \"categoryName\": \"Demo\",\n            \"tags\": []\n        }, {\n            \"id\": 308,\n            \"createTime\": \"2022-08-26T03:54:52\",\n            \"updateTime\": \"2022-08-26T03:54:52\",\n            \"author\": \"NebulaGraph\",\n            \"content\": \"Distributed database systems like NebulaGraph perform well in storing data but they make DevOps difficult and complicated. Building and managing clusters is pain and time-consuming. Not to mention it is not easy to back up and upgrade the system, especially in the production environment.\\n\\nIntroducing [NebulaGraph Dashboard](https://nebula-graph.io/products/dashboard/), a visualization tool that helps you manage your NebulaGraph clusters in an intuitive web user interface. NebulaGraph Dashboard can help DevOps engineers and database administrators (DBAs) reduce the daily cost of managing a NebulaGraph cluster and ensure the stability of their systems.\\n\\nIf you want a free trial of NebulaGraph Dashboard enterprise edition, click here: [https://dashboard.nebula-graph.io/](https://dashboard.nebula-graph.io/)\\n\\n## The structure\\n\\nThe following figure gives the six main features of NebulaGraph Dashboard. They are cluster management, monitoring, alarming, single cluster configuration, access control, and system setting.\\n\\n![nebulagraph-dashboard-structure](https://www-cdn.nebula-graph.io/nebula-website-5.0/images/demo/demo-nebulagraph-dashboard/dashboard-1.png)\\n\\n\\n## The one dashboard to rule them all\\n\\nFirst, let’s dive into the main features of NebulaGraph Dashboard: multi-cluster orchestration & lifecycle management, monitoring and alerting, and access control.\\n\\n\\n## Multi-cluster orchestration & lifecycle management\\n\\nThe lifecycle for a NebulaGraph cluster refers to the journey from the creation of a cluster to management to eventual recycling.\\n\\n![graph-database-cluster](https://www-cdn.nebula-graph.io/nebula-website-5.0/images/demo/demo-nebulagraph-dashboard/dashboard-2.png)\\n\\nNebulaGraph Dashboard can manage the full lifecycle of your NebulaGraph cluster in a visualized way.\\n\\nWhen you have not created a NebulaGraph cluster, NebulaGraph Dashboard enables you to start the creation of a new cluster. If you are already working with NebulaGraph clusters, you can import those clusters into NebulaGraph Dashboard in batch and monitor or manage them there. We will go through the entire process of creating and managing clusters in this chapter.\\n\\n## Create a cluster from Nebula Dashboard\\n\\n![create-graph-database-cluster](https://www-cdn.nebula-graph.io/nebula-website-5.0/images/demo/demo-nebulagraph-dashboard/dashboard-3.png)\\n\\n\\nThere are five steps to create a cluster, as shown in the figure above:\\n\\n1. Input a unique cluster name\\n2. Choose the NebulaGraph version that you want to create a cluster. Please note that NebulaGraph Dashboard only supports NebulaGraph v2.0 and above. By default, NebulaGraph Dashboard provides three built-in installation package: v2.6.1, v2.5.1, and v2.0.1. It will also support NebulaGraph v3.0.0 once it is released in mid February 2022.\\n3. Add nodes to the graph database cluster. You may need to authorize the step from SSH.\\n4. Select the services to be deployed on different nodes. For simplicity, you can also use the “Auto Add Service” feature to evenly deploy services to different nodes.\\n5. Confirm everything is OK and click “Create Cluster.” And voilà, you have your NebulaGraph cluster up and running.\\n\\n\\n## Importing a cluster into Nebula Dashboard\\n\\nIf you have a NebulaGraph cluster running, you can import it into NebulaGraph Dashboard to manage it in a graphic user interface.\\n\\n![graph-database-dashboard-1](https://www-cdn.nebula-graph.io/nebula-website-5.0/images/demo/demo-nebulagraph-dashboard/dashboard-4.png)\\n![graph-database-dashboard-2](https://www-cdn.nebula-graph.io/nebula-website-5.0/images/demo/demo-nebulagraph-dashboard/dashboard-5.png)\\n\\n\\n## Graph database cluster operations and maintenance\\n\\nOnce you have completed the above two steps, you can use NebulaGraph Dashboard to manage your clusters. Cluster management can be tedious. It involves repeated actions such as management of clusters, nodes and services. NebulaGraph Dashboard can free DevOps engineers from this unbearable boredom by simplifying the whole process of cluster management.\\n\\nHere are what you can do with NebulaGraph Dashboard to manage your cluster:\\n\\n***Node and service management***\\n\\nNebulaGraph Dashboard’s Node management and Service management modules will give you an intuitive overview of all the information about running nodes in the cluster and services in each node. You can do actions such as adding empty nodes and starting or stopping services in a node.\\n\\n![graph-database-dashboard-3](https://www-cdn.nebula-graph.io/nebula-website-5.0/images/demo/demo-nebulagraph-dashboard/dashboard-6.png)\\n![graph-database-dashboard-4](https://www-cdn.nebula-graph.io/nebula-website-5.0/images/demo/demo-nebulagraph-dashboard/dashboard-7.png)\\n\\n\\n***Be informed. Be alerted.***\\n\\nMonitoring the health of a cluster is the first priority forNebula Dashboard. NebulaGraph Dashboard provides cluster overview, node metrics monitoring, service metrics monitoring, and alarm notification capabilities to help DevOps engineers stay informed of their NebulaGraph cluster health.\\n\\n## Cluster overview\\n\\nIn the overview section, DevOps engineers can quickly understand the overall situation of the current cluster, including the distribution of services in the node, operational status, and customize indicators of their most concerned metrics to stay alarmed.\\n\\n![graph-database-dashboard-5](https://www-cdn.nebula-graph.io/nebula-website-5.0/images/demo/demo-nebulagraph-dashboard/dashboard-8.png)\\n\\n***Node monitoring***\\n\\nNode monitoring is used to present all the information of a particular NebulaGraph node, including its CPU usage, memory, load, and disk. You can also set a baseline for a specific metric so that you can be alerted when the metric exceeds the baseline.\\n\\n![graph-database-dashboard-6](https://www-cdn.nebula-graph.io/nebula-website-5.0/images/demo/demo-nebulagraph-dashboard/dashboard-9.png)\\n\\n\\n***Service monitoring***\\n\\nService monitoring mainly involves the metrics of three services of NebulaGraph clusters. The three services are graphd, metad and storaged. Currently, NebulaGraph Dashboard can present dozens of cluster metrics and monitor them based on their aggregations and averages. These metrics include the number of slow queries and errors in the graphd service, vertex and edge latencies of storaged, and heartbeat latencies of the metad service.\\n\\n![graph-database-dashboard-7](https://www-cdn.nebula-graph.io/nebula-website-5.0/images/demo/demo-nebulagraph-dashboard/dashboard-10.png)\\n\\n\\n***Alert***\\n\\nNebulaGraph Dashboard’s alert module is a service for alerting the monitoring indicators of NebulaGraph clusters. You can view alert information, set alert rules and alert recipients in the module.\\n\\nYou can set up alert baselines of indicators that you care about, and the frequency and duration of the alert triggering, as well as the notification message template, so that the system will automatically send out notifications to alert you when any of the indicators triggers the corresponding rules.\\n\\nYou can view history alert messages, in-site alert messages, set up email and webhook notifications, and set alert rules in the alert module. Here is how the module works:\\n\\n1. Create custom alert rules or activate alert rules.\\n2. An alert message will be sent when the system monitors any abnormal metric.\\n3. A pop-up in-site message of the alert is shown on the top right of NebulaGraph Dashboard. The system will send out an alert email if you have set an email recipient.\\n4. You can then perform troubleshooting based on the alert message.\\n\\n![graph-database-dashboard-8](https://www-cdn.nebula-graph.io/nebula-website-5.0/images/demo/demo-nebulagraph-dashboard/dashboard-11.png)\\n\\n\\n***Fine-grained access control***\\n\\nAs a multi-cluster management tool, NebulaGraph Dashboard has enabled a fine-grained access control system to ensure security. NebulaGraph Dashboard has designed two roles, and they are admins and users. While admins are authorized to perform overall configuration of the Dashboard, users are only entitled to operate within the scope that is assigned to them.\\n\\n***LDAP login***\\n\\nNebulaGraph Dashboard supports login with Lightweight Directory Access Protocol (LDAP). Once the LDAP information is provided in the deployment phase, users can log in with their enterprise account. Admins can invite users by sending them an email containing a verification link.\\n\\n## Feature roadmap\\n\\nNebulaGraph Dashboard is designed to simplify the daily tasks of DevOps engineers and database administrators (DBAs). It will support the following features in the forthcoming versions.\\n\\nBack up and upgrade a cluster with one click. This feature will only be compatible with NebulGraph v3.0 and above. Visualized management of Storage Zone TV dashboard that allows you to display key metrics in a customized way in a display in your office Slow query management that allows DBAs to quickly identify slow queries.\\n\\n\\n***Wanna try?***\\n\\nCheck out the NebulaGraph Dashboard playground to see how NebulaGraph Dashboard works in practice.. You can also click this link to request a 15-day free trial to test NebulaGraph Dashboard enterprise edition out in your own environment.\\n\\nIf you encounter any problems in the process of using Nebula Graph, please refer to [NebulaGraph Database Manual](https://docs.nebula-graph.io/3.0.1/pdf/NebulaGraph-EN.pdf) to troubleshoot the problem. It records in detail the knowledge points and specific usage of the graph database and the graph database NebulaGraph.\\n\\nJoin our [Slack channel](https://join.slack.com/t/nebulagraph/shared_invite/zt-7ybejuqa-NCZBroh~PCh66d9kOQj45g) if you want to discuss with the rest of the NebulaGraph community!\",\n            \"description\": \"NebulaGraph Dashboard is an out-of-the-box visualized cluster management tool. It enables cluster operations such as node management, service management, scaling in/out, and configuration update.\",\n            \"pic\": \"https://www-cdn.nebula-graph.io/nebula-website-5.0/images/demo/demo-nebulagraph-dashboard/graph-database-dashboard.png\",\n            \"publish\": true,\n            \"slug\": \"nebula-dashboard-playground\",\n            \"title\": \"Introducing NebulaGraph Dashboard\",\n            \"createUserId\": 8,\n            \"updateUserId\": 8,\n            \"blogCategoryId\": 2,\n            \"categoryName\": \"Demo\",\n            \"tags\": []\n        }, {\n            \"id\": 307,\n            \"createTime\": \"2022-08-26T02:45:05\",\n            \"updateTime\": \"2022-08-26T02:45:05\",\n            \"author\": \"NebulaGraph\",\n            \"content\": \"1. Visit Nebula Playground: [Game_monsters](https://explorer.nebula-graph.io/explorer)\\n2. Click on the search icon in the upper left corner and select the corresponding query method\\n3. There are 3 ways to import data.\\n\\n>Method 1: VID query, and then click \\\"Random Import\\\" button, you can import some random vertex, such as the following show VID, and then click Add.\\nmonster001\\nmonster002\\nmonster003\\nmonster025\\nmonster026\\nmonster133\\nmonster134\\nmonster135\\nmonster136\\n\\n\\n>Method 2: tag query, select tag and index, add a quantity limit, such as 50, click the plus sign, add a filtering condition condition, and then click explorer.\\n\\n>Method 3: Subgraph query, enter a vid, such as monster001, fill in 2 steps, choose monster_type for edge type, choose BOTH for direction, then click explorer.\\n\\n4. By this time the vertices should be displayed, you can double click them, or use the right sidebar to expand the conditions for edge expansion.\\n\\n## Details\\n\\nGame business scenarios often encounter relationship problems, characters and characters, characters and monsters, pets, NPCs, props, etc.\\n\\nWith the traditional form of tables, it is impossible to find the required relationships intuitively, here we take the game as an example, using publicly available data to provide everyone online experience Nebula Graph graph database.\\n\\nThe structure of this game dataset, as shown in the figure:\\n\\n***Tag: monster, property, person***\\n\\n***Vertex: 151 monsters, 9 properties, 13 characters***\\n\\n***Edge: property relationship, damage doubling, damage halving, attribution, evolution relationship***\\n\\n![pocket-monsters-graph-databases](https://www-cdn.nebula-graph.io/nebula-website-5.0/images/demo/pocket-monsters/graph-database-pocket-monsters.png)\\n\\n\\nLet's start by looking up something related to a famous 025 yellow monster.\\n\\n\\n* Go to the studio graph exploration screen.\\n* The top left corner confirms that we are currently in the Game_Monsters space (the space of the game dataset)\\n\\n![graph-database-workspace](https://www-cdn.nebula-graph.io/nebula-website-5.0/images/demo/pocket-monsters/graph-database-workspace.png)\\n\\n* Click on the search icon in the upper left corner and select the corresponding query method\\n\\n![graph-database-workspace-2](https://www-cdn.nebula-graph.io/nebula-website-5.0/images/demo/pocket-monsters/monster-4.png)\\n\\n* Enter vid: monster025\\n* Click to Add\\n\\n\\nYou can find out the vertex of the yellow charged monster number 025. (VID is monster001 - monster151)\\n\\n![monster-graph-database-1](https://www-cdn.nebula-graph.io/nebula-website-5.0/images/demo/pocket-monsters/monster-5.png)\\n\\nDouble click on the point to automatically expand all related points\\n\\n\\nFor example, you can see the vertex of a certain monster with a longer tail and a charged partner, 026, of the evolutionary relation 025.\\n\\n![monster-graph-database-2](https://www-cdn.nebula-graph.io/nebula-website-5.0/images/demo/pocket-monsters/monster-6.png)\\n\\nAt the same time click on the attribute display button, check the box to select the attributes we need to display, you can see the vertex in the figure corresponding to the value of the attribute. For example: name, attack power, defense power, etc.\\n\\n![monster-graph-database-3](https://www-cdn.nebula-graph.io/nebula-website-5.0/images/demo/pocket-monsters/monsters-7.png)\\n![monster-graph-database-4](https://www-cdn.nebula-graph.io/nebula-website-5.0/images/demo/pocket-monsters/monsters-8.png)\\n\\nIn the result of the query, we can also see that there is a vertex for the attribute, and the tag for the monster attribute, with the same name.\\n\\n> How to build the data structure often needs to be designed by the user\\n\\n![monster-graph-database-5](https://www-cdn.nebula-graph.io/nebula-website-5.0/images/demo/pocket-monsters/monsters-9.png)\\n\\nClicking on this attribute vertex will automatically expand it, at this vertex we can visually view all the monsters with electric attributes, each vertices shows the corresponding attribute value, and intuitively find all the monsters of the electric family.\\n\\n![monster-graph-database-6](https://www-cdn.nebula-graph.io/nebula-website-5.0/images/demo/pocket-monsters/monsters-10.png)\\n\\n\\nWith the right sidebar, we can also view the exact Tag of the current canvas.\\n\\n![monster-graph-database-7](https://www-cdn.nebula-graph.io/nebula-website-5.0/images/demo/pocket-monsters/monster-15.png)\\n\\n\\n## Now, let's look up the character relationships\\n\\nBy vid query enter\\n\\nperson1\\n\\nperson2\\n\\nperson3\\n\\nperson4\\n\\nperson5\\n\\nperson6\\n\\nperson7\\n\\nperson8\\n\\nperson9\\n\\nperson10\\n\\nperson11\\n\\nperson12\\n\\nperson13\\n\\n\\n![monster-graph-database-8](https://www-cdn.nebula-graph.io/nebula-website-5.0/images/demo/pocket-monsters/monsters-11.png)\\n\\n\\nFrom the results, you can see 8 big city dojo directors, former champions, and 4 major divas\\n\\n![monster-graph-database-9](https://www-cdn.nebula-graph.io/nebula-website-5.0/images/demo/pocket-monsters/monsters-12.png)\\n\\n\\nDouble click on a person's vertex to see the monsters he has\\n\\n![monster-graph-database-10](https://www-cdn.nebula-graph.io/nebula-website-5.0/images/demo/pocket-monsters/monsters-13.png)\\n\\n\\nClicking on each monster allows us to quickly expand the relationship and find the attributes that are incompatible with each other\\n\\nDouble click on the restraining attribute to quickly expand a group of monsters with that attribute, and select the monsters you have to double the damage on the boss's monsters, or select the monsters that halve the damage for defense.\\n\\n![monster-graph-database-11](https://www-cdn.nebula-graph.io/nebula-website-5.0/images/demo/pocket-monsters/monsters-14.png)\\n\\n\\n> This is a typical way to use graphs for quick game strategy search, and they have a huge advantage over the traditional table format in terms of correlations.\\n\\n\\nWe hope this public data demo is helpful for you to win the game.\\n\\nIf you are interested in the example, click Quick Start and try it out for yourself.\\n\\n[Game_monsters](https://explorer.nebula-graph.io/explorer)\\n\\n## Reference:\\n\\n[Nebula Explorer presentation](https://www.bilibili.com/video/BV1VL4y1V7C2) \\n\\n[Figure Exploration, Nebula Explorer Manual](https://docs.nebula-graph.io/3.1.0/nebula-explorer/about-explorer/ex-ug-what-is-explorer/)\\n[nGQL, CRUD manual](https://docs.nebula-graph.com.cn/master/2.quick-start/4.nebula-graph-crud)\",\n            \"description\": \"In this case, we use the game's publicly available dataset, imported into NebulaGraph Explorer, to show you how to take advantage of the NebulaGraph to demonstrate the value of the graph database.\",\n            \"pic\": \"https://www-cdn.nebula-graph.io/nebula-website-5.0/images/demo/pocket-monsters/pocket-monsters-banner.jpg\",\n            \"publish\": true,\n            \"slug\": \"game-pocket-monsters\",\n            \"title\": \"Game Pocket Monsters with NebulaGraph\",\n            \"createUserId\": 8,\n            \"updateUserId\": 8,\n            \"blogCategoryId\": 2,\n            \"categoryName\": \"Demo\",\n            \"tags\": []\n        }, {\n            \"id\": 306,\n            \"createTime\": \"2022-08-26T01:26:01\",\n            \"updateTime\": \"2022-08-26T01:27:03\",\n            \"author\": \"NebulaGraph\",\n            \"content\": \"In this dataset, we will use the visual graph exploration tool NebulaGraph Explorer to explore the publicly available data relationships in Game of Thrones.\\n## Quick start: 1.\\n\\n1. Visit the Nebula Playground here: [Game_of_thrones](https://explorer.nebula-graph.io/explore?name_space=Game_of_thrones)\\n2. Click on the “magnifying glass” icon in the upper left corner and click on “Query by VID”\\n3. Click “Random Import” to import randomly, click “ADD” to add.\\n4. You can double-click them to expand them, or use the function bar on the left to do other operations.\\n\\n## Details.\\n\\n“Game of Thrones” is a TV series that everyone likes very much, and the wonderfully complex character relationships are a very good case study for exploring people and their relationships.The traditional table format can not intuitively reflect the scale of the relationship, while the graph format can very easily reflect the complexity of the relationship, and quickly dig the relationship between the characters.\\n\\nComplexity of this dataset: ★☆☆☆☆\\n\\nThe structure of this dataset, as shown in the figure:\\n\\n***tag: person***\\n\\n***edge: relation***\\n\\n![game-of-thrones-graph-database-1](https://www-cdn.nebula-graph.io/nebula-website-5.0/images/demo/demo-game-of-thrones/game%20of%20thrones%201.png)\\nLet’s start exploring a few of the more key characters in the power trip\\n\\n* Go to Nebula Explorer, click on the magnifying glass icon in the upper left corner, and then click on the “Query by VID” button. \\n\\n![query-by-VID](https://www-cdn.nebula-graph.io/nebula-website-5.0/images/demo/demo-game-of-thrones/game%20of%20thrones%202.png)\\n\\n* At this time, click the random import button in the pop-up box, and you can already explore the canvas. \\n\\n![random-import](https://www-cdn.nebula-graph.io/nebula-website-5.0/images/demo/demo-game-of-thrones/game%20of%20thrones%203.png)\\n\\n*But in this step, we manually import some famous characters, copy the names below, and click “ADD” to add the points to the canvas. \\n\\n![query-by-VID=2](https://www-cdn.nebula-graph.io/nebula-website-5.0/images/demo/demo-game-of-thrones/game%20of%20thrones%204.png)\\n\\n```\\nDaenerys-Targaryen\\nEddard-Stark\\nJon-Snow\\nTyrion-Lannister\\nRobert-Baratheon\\n```\\n\\n* At this point we can already see several red dots corresponding to the characters displayed in the canvas, so we right-click and modify the color of each dot individually for our subsequent analysis. \\n\\n![graph-database-canvas](https://www-cdn.nebula-graph.io/nebula-website-5.0/images/demo/demo-game-of-thrones/game%20of%20thrones%205.png)\\n\\n* On the left side, select the property display option, check person.name and click OK, so that the names of all points are displayed. \\n\\n![graph-databae-canvas-2](https://www-cdn.nebula-graph.io/nebula-website-5.0/images/demo/demo-game-of-thrones/game%20of%20thrones%206.png)\\n\\n* Here we select the middle imp Tyrion-Lannister, tap the left expansion panel, configure the parameters of the panel, here we select the direction as bidirectional, customize the color of the expansion, we select the same green color as the imp, then click the expand button. \\n\\n![graph-database-canvas-3](https://www-cdn.nebula-graph.io/nebula-website-5.0/images/demo/demo-game-of-thrones/game%20of%20thrones%207.png)\\n\\n\\n*At this point we can already see the character relationships associated with this point. \\n\\n![graph-database-canvas-4](https://www-cdn.nebula-graph.io/nebula-website-5.0/images/demo/demo-game-of-thrones/game%20of%20thrones%208.png)\\n\\n* Similarly we can see the relationships of the female dragon lady, the male lead Jon-Snow, and the others. \\n\\n![graph-database-canvas-5](https://www-cdn.nebula-graph.io/nebula-website-5.0/images/demo/demo-game-of-thrones/game%20of%20thrones%209.png)\\n\\n***This is a typical way of using graphs to quickly perform a character relationship lookup. Graphs have a huge advantage over the traditional table format in terms of associative relationships.***\\n\\nWe can learn in the results, who has richer interpersonal relationships, already which people are in which camps, the size of family relationships, etc.\\n\\nDataset file: [Download address](https://github.com/yyh0808/game-of-thrones-dataset-nebula)\\n\\nRelated article: [Analyzing ＜Game of Thrones＞ character relationships with NetworkX + Gephi + NebulaGraph](https://blog.csdn.net/weixin_44324814/article/details/108100159)\",\n            \"description\": \"In this dataset, we will use the visual graph exploration tool Nebula Explorer to explore the publicly available data relationships in Game of Thrones.\",\n            \"pic\": \"https://www-cdn.nebula-graph.io/nebula-website-5.0/images/demo/demo-game-of-thrones/game-of-thrones-banner.png\",\n            \"publish\": true,\n            \"slug\": \"game-of-thrones\",\n            \"title\": \"Game of Thrones Character Relationships\",\n            \"createUserId\": 8,\n            \"updateUserId\": 8,\n            \"blogCategoryId\": 2,\n            \"categoryName\": \"Demo\",\n            \"tags\": []\n        }, {\n            \"id\": 305,\n            \"createTime\": \"2022-08-03T10:37:00\",\n            \"updateTime\": \"2022-08-03T10:37:00\",\n            \"author\": \"Hao Wen\",\n            \"content\": \"This year's SIGMOD'22 was held in Philadelphia, and I was honored to attend as a speaker on behalf of the company. During this conference, I communicated a lot with students, professors, and companies. On the one hand, I publicized our products; on the other hand, I got many valuable suggestions and opinions. Next, I'll share some of these suggestions, as well as my overall thinking.\\n\\n## Graph learning and storage technology have attracted much attention\\n\\n‍\\n\\n**There are two hotspots that most participants pay attention to at this conference, which reflects the new trend in the industry.&nbsp;**\\n\\n**First, the importance of machine learning is further increasing, and graph learning is becoming a new hotspot**. In recent years, more and more conferences that focus on specific systems set sections about machine learning to study the use of machine learning in optimizing systems or algorithms. Among the sub-fields of machine learning, a new force, i.e., graph learning, which is machine learning on graphs, suddenly rises. Just like other sub-fields of machine learning, graph learning also aims at solving problems in the real world. **Through the method of embedding, graph learning extracts feature vectors from a graph, and then uses machine learning methods to solve complex problems in the graph, including classification, subgraph matching, and link classification.**\\t\\t\\n\\nJudging by the tendency of conferences and guidance from the National Science Foundation (NSF), this trend is likely to continue. We have also received a lot of inquiries from students and professors about whether NebulaGraph supports graph learning. From a graph database provider's perspective, graph learning is one of the NebulaGraph application scenarios. We should provide not only the graph database core, but also libraries, interfaces, and even optimizations for upper-layer applications to ensure the smooth and efficient operation of upper-layer applications. On the other hand, if NebulaGraph can help academia make some breakthroughs in areas like graph learning, it'll be great for our company, and the industry as a whole.\\n![](http://www-cdn.nebula-graph.io/nebula-website-5.0/images/blogs/thoughts-after-attending-the-sigmod22-conference/figure1.png)\\n\\n**In addition to machine learning, I think another hotspot is memory, including in-memory databases, persistent memory, etc**. Non-volatile memory, or persistent memory, has become more and more mature through the joint efforts of academia and industry for more than ten years. Research on how to use persistent memory is also very hot, and there are numerous articles on it at top events of almost all systems.\\n\\n**In the database field specifically, research topics include how to design an in-memory database, how to mix durable memory with SSD, DRAM, and other types of storage, and how to solve the problem of memory segregation.** NebulaGraph, as a graph database provider, should be embracing the changes that new hardware brings, and exploring persistent memory-based storage systems as well.\\n\\n## The advantages and development direction of the graph\\n\\n**In addition to the new trends, my biggest takeaway and thought from the conference was the comparison between graph databases (GDBMS) and relational databases (RDMBS). **There were quite a few people at the conference who were wondering why the graph database was not implemented based on a relational database, because relational databases have been optimized over the years, and they are really armed to the teeth. For example, Professor Peter Boncz (one of the founders of LDBC) gave a keynote address at the EDBT/ICDT conference earlier this year on The (Sorry) State of Graph Database Systems [1]. He stated that compared with RDBMS, GDBMS still had a lot to improve in terms of computation and storage, especially regarding AP queries like subgraph matching. He proposed a performance benchmark for subgraph matching scenarios. In his experimental environment, none of the graph database software came close to the performance of relational databases Hyper and Umbra in the subgraph matching scenario (see the following figure).\\n![](http://www-cdn.nebula-graph.io/nebula-website-5.0/images/blogs/thoughts-after-attending-the-sigmod22-conference/figure2.png)\\n\\nCertainly, the setup of this experiment is worthy of discussion. Still, Peter, as a founder of LDBC, did not mean to disparage graph databases but to encourage graph database providers to improve their products by using subgraph matching, a query scenario in which graph databases are not well optimized.\\n\\nFrom the broader picture, NebulaGraph, as a graph database provider,** must know the differences between graph databases and relational databases and grasp the advantages of graph databases**. For example,** GQL is easier to use and more efficient when executing graph queries than SQL**. Emil Eifrem, CEO of Neo4J, has a perfect example [3]: For a very common query in an AP system, if using SQL, it needs a very long statement consisting of 23 SELECTs, 21 WHEREs, 11 JOINs, and 9 UNIONs. But if you use a graph query language, you only need a MATCH and a WHERE. For SQL, the query task may be impossible to complete or easy to go wrong, but for a graph query language, the task is easy to complete, saving countless manpower and resources.\\n\\n**In terms of performance**, in addition to the natural advantages of graph databases over relational databases in association relation queries, i**t is also necessary to gain an advantage over relational databases in a variety of other graph-related queries (not limited to subgraph matching mentioned above). **This is actually the core enlightenment of Peter's proposal of subgraph query as a benchmark for us.\\t\\t\\n\\n‍\\n\\n## Thoughts about the underlying storage structure\\n\\n![](http://www-cdn.nebula-graph.io/nebula-website-5.0/images/blogs/thoughts-after-attending-the-sigmod22-conference/figure3.png)\\n\\n\\nAt last, we also gained a number of suggestions for the underlying storage. NebulaGraph currently applies RocksDB, which is based on LSM-Tree, as the underlying storage engine. However, it has been controversial whether LSM-Tree is suitable for the workload of the graph database, especially, whether it is the best choice with the lowest cost on the cloud. Possible alternatives are:\\n\\n- Bε-tree File System, or betrFS.[ https://www.betrfs.org/](https://www.betrfs.org/)\\n\\n- LiveGraph. [https://marcoserafini.github.io/papers/LiveGraph.pdf](https://marcoserafini.github.io/papers/LiveGraph.pdf)\\n\\n- B+ Tree. We will follow up the research on this aspect.\\t\\n\\nAll in all, the SIGMOD'22 trip was fruitful. Look forward to SIGMOD next year. See you in Seattle!\\n\\n[1] [https://conferences.inf.ed.ac.uk/edbticdt2022/?contents=invited_talks.html](https://conferences.inf.ed.ac.uk/edbticdt2022/?contents=invited_talks.html)\\n\\n[2] [https://docs.google.com/presentation/d/1xUbooiL8rIMzkp9G9EXmN4tIMMWt2mC53Q0u4mslq5g/edit#slide=id.ge0f13c0edc_0_47](https://docs.google.com/presentation/d/1xUbooiL8rIMzkp9G9EXmN4tIMMWt2mC53Q0u4mslq5g/edit#slide=id.ge0f13c0edc_0_47)\\n\\n[3] [https://people.eecs.berkeley.edu/~istoica/classes/cs294/15/notes/21-neo4j.pdf](https://people.eecs.berkeley.edu/~istoica/classes/cs294/15/notes/21-neo4j.pdf)\\n\\n‍\",\n            \"description\": \"This year's SIGMOD'22 was held in Philadelphia, and I was honored to attend as a speaker on behalf of the company. During this conference, I communicated a lot with students, professors, and companies. On the one hand, I publicized our products; on the other hand, I got many valuable suggestions and opinions. Next, I'll share some of these suggestions, as well as my overall thinking.\",\n            \"pic\": \"https://www-cdn.nebula-graph.io/nebula-website-5.0/images/blogs/thoughts-after-attending-the-sigmod22-conference/banner.jpg\",\n            \"publish\": true,\n            \"slug\": \"thoughts-after-attending-the-sigmod22-conference\",\n            \"title\": \"Thoughts after attending the SIGMOD'22 conference\",\n            \"createUserId\": 1,\n            \"updateUserId\": 1,\n            \"blogCategoryId\": 1,\n            \"categoryName\": \"Blog\",\n            \"tags\": []\n        }, {\n            \"id\": 304,\n            \"createTime\": \"2022-07-12T13:56:00\",\n            \"updateTime\": \"2022-07-12T13:56:00\",\n            \"author\": \"NebulaGraph\",\n            \"content\": \"# Financial Fraud Detection: One of the Best Practices of Knowledge Graph\\n\\nHello, I'm Qingxin Kong from Bangsun Technology. I'm glad that I have this opportunity to share the topic **Financial Fraud Detection: Best Practices of Knowledge Graph **with NebulaGraph community members.\\n\\n### Background: Problems with traditional fraud detection solutions\\n\\nFinancial frauds remain a hot topic in recent years. There's a new trend emerging that fraudsters are organized and ganged up on. It is difficult to verify whether the customer's information provided is fake or not. Such inconsistent information makes it difficult to identify the real frauds and leads to a lot of problems. The bank may control the passing rate by raising the requirements to handle loan applications due to the uncertainty of the customer's information, which is effective but brings losses.\\n\\nThe 2022 government work report said that financial inclusion will be further expanded and the customer base will include rural customers, which means there will be \\\"white accounts\\\" having no credit reports. Traditional fraudulent behaviors include fake credit reports, commission agencies, and collusion. \\n\\nSome agencies focus on a few banks for credit card applications or loans. Those agencies know the bottom line and red line of the banks. As long as the bottom line is not touched, they can successfully apply for loans. Those agencies may even know how to make the loan amount higher.\\n\\nTraditional fraud detection solutions rely heavily on expert rules, that is, risk control experts set a batch of rules through experience and then adjust the trigger conditions to find a business balance between the pass rate and rejection rate. It costs little to forge things like ID numbers and bank card flows. There are no good means of control for mass and high-volume frauds.\\n\\n### Empowering graphs: Paths to intelligent fraud detection solutions\\n![](http://www-cdn.nebula-graph.io/nebula-website-5.0/images/blogs/financial-fraud-detection-one-of-the-best-practices-of-knowledge-graph/figure1.png)\\n\\nLet's take a look at the development path of intelligent fraud detection solutions. \\n\\nThe first step is to build expert rules, but it is difficult to cover all risks through expert experience. The second step is to build a machine learning model to prevent and control the overall risk and then use expert rules to help the model. Expert rules and machine learning models well complement each other. However, based on the current experience, we may face lots of problems when building the machine learning model. The first reason is insufficient data and the second is the limited growth of the sample data. Because there are probably be 6~7 ways to apply for loans or credit cards, manual inspection is required in the end, so the data growth is very low. There will be a case below to explain the problem.\\n\\nTherefore, we need to use relational graphs to crack the problems of information inconsistency. Both group and agency frauds have a batch of application behaviors. We can use the visualized graphs to recognize fraud samples. In this way, we may capture dozens to hundreds of fraudulent applications. It is also a very effective way for sample replenishment. Graphs can also be used for consistency checks. The multiple sources of information prove the information inconsistency. Graphs are displayed from the dimension of relations which helps us to do secondary verification. For example, multiple applications sharing the same information like the same phone number and IP address are considered group fraud.\\n\\nTo detect these group frauds, we use graphs that help us find rules of relations. These rules are complementary to the original expert rules. On the other hand, we can extract graph information to optimize machine learning patterns which increases the pattern's ability to detect group frauds and label more suspicious objects. Therefore, the expert rules, machine learning models, and graphs construct an effective ecological closed loop and complement each other to detect frauds. \\n\\nIn terms of the specific applications of relational graphs, I've listed the following applications:\\n\\n**_Application Graph_**: An example is using graphs to detect fraudulent applicants for loans and credit cards. One characteristic of a group is that information is shared by the group members. From the visual queries of graph data, a lot of correlations will be found. The more common information includes identity card, cell phone number, device fingerprint, e-mail address, etc. We use this information to explore relationships among these data, and then visually analyze the specific fraudulent techniques.**_‍_**\\n\\n**_Transaction Graph_**: For multi-level transfer relationships, we mainly focus on the final flow of funds and who are the beneficiaries. The recent gambling fraud issue is the main focus of concern for financial supervisors, which is also one of the main directions we focus on. We have many customer cases. Through the list of suspects provided by the supervision of our internal rule models, suspicious transactions can be detected. In this process, we use historical flow data to explore more layers of data starting with the collected accounts. We may also use the graph exploration feature or a graph platform to analyze existing data for incremental prevention and control. \\n\\n**_Enterprise Graph/Internal Control Graph_**: Nowadays, there are interconnections between two or more enterprise businesses. The impact of external risks on the enterprise is increasing. Using graphs can outline the whole picture of risks for an enterprise. Combined with the introduction of external risks, the impact of external risks on the enterprise can be predicted as early as possible. Internal control graphs are more often used for detecting ethical or operational risks, such as misuses or failure to do certain operations in accordance with rules and regulations. Moral risks occur more often in unusual financial transactions between employees and enterprises, such as illegal fundraising or misappropriation of funds. The prevention core is to use graphs to check the actual control accounts of employees. Because bank employees are well acquainted with the bank's business. Few employees take their own accounts for some crimes. The identification of these control accounts can be explored by the means of graphs. For example, the accounts of relatives. We have a case of a joint-stock bank. In this case, the target was located in the account of the employee's girlfriend, because from visual graphs we found that many funds actually flowed to the girlfriend's account. It is very difficult to detect a case like this by traditional means.\\n\\n**_Money Laundering Graph_**: The money laundering problem has existed for a long time. There are many derivative money laundering ways in different periods. Popular ways include the underground money bank and the \\\"order sneaking\\\" platform. Criminals hide their money on the platform where order-sneaking customers' daily consumption transactions are normal, however, there is actually a fixed pattern on visual graphs to detect cases like this. So how creating business relations with graphs and making graph rules to perform real-time monitoring and detections have become an important task for Bangsun, taken as a graph technology platform, to empower business developers. We will talk about it later. \\n\\n**_Vehicle Insurance/Ops Graph_**: For the vehicle insurance/ops field, we combine the initialization network with the design of vertices and edges, then query for a subgraph where the data is relatively appropriate and related, and then combine the experience of business experts to perform some exploratory and analytical work.\\n\\n### **Use case 1: Flow of funds**\\n![](http://www-cdn.nebula-graph.io/nebula-website-5.0/images/blogs/financial-fraud-detection-one-of-the-best-practices-of-knowledge-graph/figure2.png)\\n\\nHere, some specific application scenarios are also briefly listed. The first one is the flow of funds after the loan is approved.\\n\\nWe can focus on business and consumer loans through graphs, and track whether the approved loans flow into the housing market or the stock market. In this process, we will use the penetration ability of graphs to calculate the amount and proportion of transferred funds. Thereafter, we can easily find risks and violations and provide a basis for investigation. The regulators currently follow the principle of \\\"three approaches and one guideline\\\" for internet credits, so we establish real-time risk control through graphs and focus on the fund flows.\\n\\nNext, we can show the specific methods. For example, loans flow directly into the blacklisted accounts or their own accounts are transferred to an account not under their name, but the account may be a controlled account. The account is used for investment and financial management. These can be analyzed with graphs and the specific pattern of the funds because the fund flows will form a very obvious distribution or pattern on visual graphs.\\n\\nThis pattern is difficult to portray with the previous expert rules, because the expert rules can only find a one-dimensional relationship, up to two-dimensional relationships, while the graphs are more adept at multi-dimensional relationship detecting. In massive transaction structures, we can also find some patterned structures. For example, the above figure shows 4-5 vertices, which involve some centralized transfer in, decentralized transfer out, and chain transaction structures like tree branches and ant nests, which are all abnormal funding patterns. Through visual graphs and combination with business knowledge, in many cases, we can see that it is problematic at a glance. As for where the specific problem is, we can check it through some functions provided by the graph, such as the k-degree query or the discovery algorithm of important nodes.\\n\\n### **Use case 2: Insurance frauds** \\n![](http://www-cdn.nebula-graph.io/nebula-website-5.0/images/blogs/financial-fraud-detection-one-of-the-best-practices-of-knowledge-graph/figure3.png)\\n\\nThe second use case is frauds in insurance, such as vehicle insurance or health insurance. The primary vehicle insurance issues are collusive claim frauds. By monitoring and associating shared identity information, it is easier to find group members and excludes irrelevant data. As for health insurance, we generally focus more on the improper relationship between doctors and patients or providers. Once there is a large number of discrete values on a drug or disease, it represents fraud. This is the common way to detect individual fraud in the past. Now the group fraud problem is severe. Group fraudsters have a shift in thinking that dozens of accounts buy the same drug through individual payment accounts. There are also aggregation accounts and many other commonalities. Combined with graph analysis and algorithms, those group frauds can be detected.\\n\\n### **Summary: Advantages of graph fraud detection**\\n![](http://www-cdn.nebula-graph.io/nebula-website-5.0/images/blogs/financial-fraud-detection-one-of-the-best-practices-of-knowledge-graph/figure4.png)\\n\\nThe advantages of graph fraud detection mainly include the following four aspects.\\n\\nAssociation analysis: This means that people take the initiative to explore and discover suspicious features, and then use graphs to make some visual associations. Business developers take the initiative to use graph tools for visual exploration and then combine the features of different business areas, such as money laundering, credit card applications, and loan applications to find some suspicious points. The suspicious points are certainly different in different areas, but the business developers who often use graph tools have a certain sensitivity to such suspicious points.\\n\\nGraph rules: This aspect is more for incremental monitoring. Unlike the previous expert rules, graph rules use the means of graph exploration associated with knowledge comparison to draw some conclusions. We conclude graph rules from the dimension of relationships, and then the system uses the rules to detect risks.\\n\\nPattern analysis: We recently called the same model, because many fraud patterns have a solidified pattern, such as cash-out from credit cards. After analyzing these solidified patterns, we convert the pattern in a graph query language into a graph library for a traversal query. Compared to the graph rules introduced above, it is a method for detecting existing data.\\n\\nCommunity analysis: As mentioned above, insufficient data is lacking for modeling graph data, the community analysis is actually a very good feature to discover fraudulent groups by algorithms and combination with data labels.\\n\\nThe above four graph fraud detection advantages can help us detect frauds in real-time, on time, and afterward. They are both the main graph functions for anti-frauds and are considered our strengths. These advantages are currently applied by about dozens of our customers.\\n\\nThis is all we have for today's article! Please look forward to the next topic Introduction of Bangsun Knowledge Graph Platform and Practical Cases of State-owned Banks.\\n\\n‍\",\n            \"description\": \"A knowledge Graph specialist from Bungsun shares his best Practices of Knowledge graph in the Financial Fraud detection field.\",\n            \"pic\": \"https://www-cdn.nebula-graph.io/nebula-website-5.0/images/blogs/financial-fraud-detection-one-of-the-best-practices-of-knowledge-graph/banner.jpg\",\n            \"publish\": true,\n            \"slug\": \"financial-fraud-detection-one-of-the-best-practices-of-knowledge-graph\",\n            \"title\": \"Financial Fraud Detection: One of the Best Practices of Knowledge Graph\",\n            \"createUserId\": 1,\n            \"updateUserId\": 1,\n            \"blogCategoryId\": 1,\n            \"categoryName\": \"Blog\",\n            \"tags\": []\n        }, {\n            \"id\": 303,\n            \"createTime\": \"2022-06-27T10:03:00\",\n            \"updateTime\": \"2022-06-27T10:03:00\",\n            \"author\": \"Wayne Sheng\",\n            \"content\": \"Real-time recommendation is a very popular topic in the retail industry. The goal of real-time recommendation is to provide personalized recommendations for users immediately after they search for products on e-commerce websites. It is a challenging task since the user has not yet decided which product they want to buy, so it requires a deep understanding of their preferences and behaviors.\\n\\nIn order to achieve this goal, real-time recommender systems need to be able to process big data at near real-time speed. Real-time systems are often used as part of e-commerce websites or mobile apps, where they can recommend products based on what the user is currently doing. For example, if an e-commerce site is selling shoes, it might want to show the user different styles of shoes when they are looking at the \\\"men's sneakers\\\" page.\\n\\nGraph technology is a good choice for real-time recommendation. It has the ability to predict user behavior and make recommendations based on it. Graph databases like Nebula Graph provide a flexible data model that allows you to represent any kind of relationship between entities. This includes not only the typical \\\"product\\\" and \\\"user\\\" relationships, but also any other relationships that are important to your application. For example, you can use a graph database to represent complex groupings of users (such as \\\"friends\\\") or categories of products (such as \\\"books\\\"). As long as there's an edge between two entities, you can use graph technology.\\n\\nThe ability to represent any kind of relationship also makes it possible for graph databases to be more expressive than relational databases. For example, if you have a large number of users who share similar interests but belong to different groups, then in relational databases this might mean having a separate table for each group and then joining them together into one big table at query time. In contrast, in a graph database, it would be possible for each user to have multiple edges representing their interests in different groups.\\n\\n## **What is a graph database?**\\n\\nA graph is known as a diagram that illustrates a relationship between two things. Thus, a graph database can be assumed to be a database to understand relationships between data.\\n\\nGraph databases are unlike traditional Relational Database Management Systems (RDBMSs) most people are familiar with. RDBMSs house data in a table of columns and rows with little or no relation to each other.\\n\\nGraph databases are fundamentally designed with a focus on the relationship between data sets. So, they require more intense and specialized processing capabilities. As a result, graph databases like Nebula Graph use sophisticated design and architecture.\\n ![](https://www-cdn.nebula-graph.io/nebula-website-5.0/images/blogs/use-cases-of-graph-databases-in-real-time-recommendation/figure1.png)\\n\\n\\nGraph databases are designed to uncover important relationships between many big data sets. In other words, it can connect the dots between multiple datasets that might otherwise just be sitting useless in silos. And graph databases are already widely used for such purposes.\\n\\n## **What is a real-time recommender system?**\\n\\nrecommender systems are used to predict customer preferences and provide them with products or services that they might like. It helps companies reduce costs and increase revenues by providing personalized recommendations for their customers.\\n\\nRecommender systems are used in a variety of industries including retail, media and entertainment, travel, and the public sector.\\n\\nFor example, Netflix uses an algorithm to recommend movies based on what you’ve watched previously. Amazon uses recommendations to help shoppers find new products they might be interested in purchasing. And Facebook uses its own algorithm to show you relevant ads based on your interests and likes.\\n\\n## **Why should you use Nebula Graph for real-time recommendation?**\\n\\nNebula Graph is a highly performant linearly scalable graph database available for use via a shared-nothing distributed model. In several real production environments, data mining and technical performance has been proven to beat the performance of competing graph databases by multiple times over.\\n\\nThe goal behind Nebula Graph is to unleash the power of exponentially growing connected data.\\n\\nNebula Graph securely processes data sets of at least twice the size and twice as fast as any competing graph databases\\n\\n*   It is the only database that can store and process billions of data points with trillions of relational connections\\n*   Nebula Graph is designed for scalability and recovery without disruption, ensuring the best business continuity available\\n   ![](https://www-cdn.nebula-graph.io/nebula-website-5.0/images/blogs/use-cases-of-graph-databases-in-real-time-recommendation/figure2.png)\\n\\n## **How graph technology is used in recommendation**\\n\\nRecommender systems aim to provide users with personalized recommendations based on their tastes and preferences. The characteristics of a user can be represented by their preferences for movies and books, for example, or their shopping habits on an e-commerce website. There are two main types of recommender systems: 1) collaborative filtering methods and 2) model-based methods.\\n\\nA collaborative filtering method, such as KNN, can predict the movie rating without knowing the attributes of the movies and users. To address this challenge, the graph factorization approach combines the model-based method with the collaborative filtering method to improve prediction accuracy when the rating record is sparse.\\n\\nGraph factorization methods have been widely used in many online recommender systems. Graph factorization is a graph-based model that can be used to represent user preferences as well as the relationship between users, items, and attributes. The goal of graph factorization is to extract the latent features from user ratings and recommendations so that these features can be used to predict users' preferences in an unsupervised manner.\\n\\nGraph factorization is done by breaking down the original dataset into smaller datasets or clusters. This process can be done using graph databases because they are designed to support highly connected data structures and relationships between data points.\\n\\n## **How to build a recommender system using Nebula Graph?**\\n\\nIf you want to build your own recommender system, there are many different approaches you can take. One of the most popular is using a graph database like Nebula Graph. This section will walk through how to build a recommender system using a graph database.\\n\\n**Define the data model**\\n\\nIn this step, we define the data model for our recommender system. The first thing we need to define is what type of data can be recommended. For example, we want to recommend movies so we will use movies as our entity. The second thing is what kind of information do we need about a movie? For example, if you are building a recommender system for books, maybe you need to know what genre it belongs to or what language it was written in. But in our case, all we need is the title, the year of release, the genre, and the country. So our movie entity is defined like this:\\n\\n‍\\n![](https://www-cdn.nebula-graph.io/nebula-website-5.0/images/blogs/use-cases-of-graph-databases-in-real-time-recommendation/figure3.png)\\n\\n‍\\n\\n**Define relationships**\\n\\nThe next thing we need to do is define what type of relationship exists between movies and users (or people). There are two types of relationships that exist between movies and users: user likes movie and user has watched movie (or user has not watched movie). In this example, we will simplify it to just user likes movie.\\n\\nNow imagine two users James and Kelsey, who are both fans of Sci-Fi Movie A, which has a genre of Science fiction. James also likes Sci-Fi Movie B, but Kelsey hasn’t watched it, so we don’t know whether Kelsey likes it or not.\\n![](https://www-cdn.nebula-graph.io/nebula-website-5.0/images/blogs/use-cases-of-graph-databases-in-real-time-recommendation/figure4.png)\\n\\nBut using this graph modeling of data, we may easily find out that Kelsey may like Sci-Fi Movie B. The recommender system would recommend Sci-Fi Movie B to Kelsey because James — who likes the same things as Kelsey — likes Sci-Fi Movie B.\\n![](https://www-cdn.nebula-graph.io/nebula-website-5.0/images/blogs/use-cases-of-graph-databases-in-real-time-recommendation/figure5.png)\\n\\n**Making recommendations**\\n\\nThere are a few graph algorithms that you can use to make recommendations within a graph database.\\n\\n**The PageRank algorithm**: The [PageRank algorithm](http://pi.math.cornell.edu/~mec/Winter2009/RalucaRemus/Lecture3/lecture3.html) is used to rank web pages in search results. The purpose of this algorithm is to determine which web pages should be displayed first when someone searches Google or any other major search engine.\\n\\nThe basic idea behind PageRank is that if you link to another page, then that page is more important than your own page. So if you are linking to a page about shopping for shoes, and someone else links to your shoe shopping site, then that person thinks your site is more important than it really is.\\n\\nIf everyone is linking to the same sites, then the people who link most get more weight.&nbsp;\\n\\nContinuing with the movie analogy, while watching movies one can either carry on watching movies of a similar genre (and hence follow their most expected journey), or skip to a random movie in a totally different genre. It turns out that this is exactly how Google ranks websites by popularity using the _PageRank_**_ _**algorithm.\\n\\nThe popularity of a website is measured by the number of links it points to (and is referred from). In our movie use case, the popularity is built as the number hashes a given movie shares with all its neighbors.\\n\\n**Collaborative filtering:** Collaborative Filtering (CF) is a method for recommender systems based on information regarding users, items, and their connections. Recommendations are made by looking at the neighbors of the user at hand and their interests. Since they are similar, the assumption is made that they share the same interests.\\n\\nRead more: [Build a Recommendation Engine With Collaborative Filtering](https://realpython.com/build-recommendation-engine-collaborative-filtering/)\\n\\n## Conclusion\\n\\nIn conclusion, it's worthwhile to use graph databases to build real-time recommender systems.&nbsp; Since graph databases are well designed for representing the relationship among users and products in the recommender system, they can be used to build real-time recommendation systems. Graph databases' support of powerful data analysis makes it useful in constructing recommender systems that need to take into account users and products with different preferences.\",\n            \"description\": \"Why graph database is a good choice for recommendation engines? How to implement a recommender system using graph technology? How graph algorithms like PageRank and Collaborative filtering are used in real-time recommendation?\",\n            \"pic\": \"https://www-cdn.nebula-graph.io/nebula-website-5.0/images/blogs/use-cases-of-graph-databases-in-real-time-recommendation/banner.jpg\",\n            \"publish\": true,\n            \"slug\": \"use-cases-of-graph-databases-in-real-time-recommendation\",\n            \"title\": \"Use cases of graph databases in real-time recommendation\",\n            \"createUserId\": 1,\n            \"updateUserId\": 1,\n            \"blogCategoryId\": 1,\n            \"categoryName\": \"Blog\",\n            \"tags\": []\n        }, {\n            \"id\": 302,\n            \"createTime\": \"2022-06-23T14:34:00\",\n            \"updateTime\": \"2022-06-23T14:34:00\",\n            \"author\": \"Nebula Graph\",\n            \"content\": \"Nebula Graph, a leading graph database technology provider, today announced that it has joined the Linked Data Benchmark Council (LDBC) as an organizational member. LDBC is the official platform for international benchmarking of graph and RDF data.\\n\\nNebula Graph’s move to join the LDBC means it is taking another step toward participating in developing graph industry standards and specifications such as Graph Query Language (GQL), an upcoming International Standard language for property graph querying that is currently being created, as well as a series of linked data benchmarks. Nebula Graph is also to play a constructive role in data evaluation guidelines across multiple industries including finance, manufacturing, and internet of things.\\n\\n“With the ongoing digital transformation, more and more enterprises around the world have large linked data that consists of hundreds of billions of vertices and edges. The LDBC sets global standards for evaluating graph database vendors’ ability to handle those data. Nebula Graph is proud to be part of the LDBC’s organizational members and we are excited about the prospect of working on global graph database standards and promoting the establishment of graph query language standards with the LDBC,” said Nebula Graph founder and CEO Sherman Ye.\\n\\nThe LDBC is a consortium comprised of industry-leading companies and organizations in the database management system industry. Their objective is to “establish an industry-neutral entity for developing graph and RDF benchmarks, auditing benchmark results, and publishing audited results.” \\n\\nThe LDBC’s benchmark tests help graph database vendors identify weaknesses in their current architectures, identify problems in how they implement queries, and scale to solve common business problems. They can also help enterprises vet the performance of databases in a way that is relevant to common business problems they want to address.\\n\\nGraph databases are involved in mapping the relationships between entities such as organizations, people, and transactions. Such technology could facilitate rapid contextualization for decision-making in 30% of organizations globally, according to market research firm Gartner.\\n\\nNebula Graph is an open-source graph database developed by Vesoft Inc. It is the world’s only large-scale graph database solution with a latency of milliseconds. Hundreds of enterprises around the world including Tencent, Meituan, JD Digits, and Kuaishou are already leveraging Nebula Graph technologies to boost their graph data processing capabilities.\\n\\n## About Vesoft Inc.\\n\\nVesoft Inc. is the creator of Nebula Graph, the world’s most capable database for big data analytics discovery. Nebula Graph provides an industry-leading capability of storing and handling billions of vertices and trillions of edges, with just milliseconds of latency. Its shared-nothing deployment architecture removes any single point of failure and allows fast recoveries, enabling the industry’s best business continuity.\\n\\nIn June 2020, Vesoft secured $8 million Series Pre-A funding from investors led by RedPoint China Ventures and Matrix Partners China, which previously contributed as an angel investor. For more information, please visit [https://nebula-graph.io/](https://nebula-graph.io/).\",\n            \"description\": \"Nebula Graph, a leading graph database technology provider, today announced that it has joined the Linked Data Benchmark Council (LDBC) as an organizational member. \",\n            \"pic\": \"https://www-cdn.nebula-graph.io/nebula-website-5.0/images/blogs/nebula-graph-joins-ldbc/banner.jpg\",\n            \"publish\": true,\n            \"slug\": \"nebula-graph-joins-ldbc\",\n            \"title\": \"Nebula Graph joins LDBC to further the development of graph database international standards\",\n            \"createUserId\": 1,\n            \"updateUserId\": 1,\n            \"blogCategoryId\": 1,\n            \"categoryName\": \"Blog\",\n            \"tags\": []\n        }, {\n            \"id\": 301,\n            \"createTime\": \"2022-06-20T12:09:00\",\n            \"updateTime\": \"2022-06-20T12:09:00\",\n            \"author\": \"Jerry Liang\",\n            \"content\": \"In the previous article of this series, \\\"[A deep dive into Azure’s billing system](https://nebula-graph.io/posts/a-deep-dive-into-azure-marketplace-billing-system)\\\", we talked about the billing and listing of your SaaS offer. In fact, in addition to SaaS services, Azure Marketplace has other service models. This article will use the practical experience of Nebula Graph as an example to talk about several ways that are suitable for combining technical products like graph databases with cloud vendors, here I distinguish them as:\\n\\n*   SaaS Fully managed service\\n*   Azure Application Self-service provisioning\\n*   Managed Service\\n\\nThe above-mentioned service types have their corresponding similar service models in all major cloud vendors and are not unique to Azure, so this article is a general discussion of solutions separate from cloud vendor platforms.\\n\\n## SaaS Fully managed service\\n\\nFully managed service is the most widely known cloud service model, where the service provider manages the machine and service resources, assumes all operations and maintenance responsibilities, guarantees service stability, and provides customers with an out-of-the-box experience. It has the following advantages.\\n\\nProvide SaaS services, allowing users to build their own business quickly out of the box without the effort of service operation and maintenance\\n\\nWith a fully managed cloud service model, resources and manpower are allocated in a reasonable manner, making cloud services more advantageous in terms of the overall cost.\\n\\n1.  Software providers usually pre-purchase resources in bulk and can bargain with the cloud vendor for resources based on the advantage of high volume, and the price of obtaining the same amount of computing resources is usually more affordable than what the average user sees when purchasing.\\n2.  Cloud service users, considering that they don’t need additional investment in operation and maintenance effort, essentially save their own costs.\\n\\nProvide a full set of solutions, supporting products are better integrated and easier to use, and make it easier for users to get started. And with the volume of fully managed cloud services, it is often easier to receive comprehensive feedback and develop a more universal solution.\\n\\nWith the above advantages, users of fully managed SaaS services generally only need to think about:\\n\\n1.  Whether they trust the software vendor enough to be willing to leave the machine and service resources to the vendor to manage their DevOps.\\n2.  Weighing this against their own situation, they conclude that using cloud services is a more cost-effective solution.\\n      ![](http://www-cdn.nebula-graph.io/nebula-website-5.0/images/blogs/azure-marketplace-three-cloud-service-models-for-isvs/figure1.png)\\n\\n## Azure application self-service provisioning\\n\\nCompared to fully managed SaaS services, self-service provisioning is actually similar to the traditional On-Premise model, except that it uses machine resources on the cloud and is more closely integrated with the cloud vendor. The software vendor can quickly help users automatically request machine resources to create standalone services on the cloud by defining deployment templates for the services and the corresponding execution scripts. This service model has the following benefits.\\n\\n1.  Compared with the traditional On-Premise model, the self-service provisioning model is better integrated, integrating the steps of machine resource preparation, service deployment and network allocation, saving the installation and deployment time of users or delivery implementers and standardizing the delivery process. And it is closely integrated with the cloud vendor, relying on the cloud vendor platform commonality function to facilitate daily resource control and observation.\\n2.  Compared to the fully managed SaaS model, the self-service provisioning model often uses the BYOL (Bring your own license) model of payment. Users pay the cloud vendor for the resources used to run the service, and then buy a software license from the software provider to activate the service. The overall fee is generally higher than fully managed, but the advantage is that the data and machine resources are controlled by the user, which is suitable for customers who have extremely high requirements for data security and do not allow data to be stored in a third party.\\n\\nSince the machine and data resources exist under the user's private cloud VPC, when the service is deployed, the operation and maintenance of the subsequent service need to be borne by the user. Even if vendors can be invited to provide operation and maintenance by purchasing consulting services, they cannot respond as quickly as fully-managed services.\\n![](http://www-cdn.nebula-graph.io/nebula-website-5.0/images/blogs/azure-marketplace-three-cloud-service-models-for-isvs/figure2.png)\\n\\n\\n## Managed service\\n\\nThe managed service model is a compromise between a fully managed cloud service and a self-service model, based on a hypothetical scenario: let the machine and data services remain in the user's VPC, and the machine and service operations and maintenance responsibilities to the software vendor. The integrity of cloud vendors’ intermediate operations and maintenance is either guaranteed by audit logs provided by them or by third-party security assessment agencies.\\n\\nHere is how the managed service model is realized in Azure:\\n\\n*   The deployment of services is similar to the self-service model, where the software vendor defines the deployment templates and corresponding scripts for the services, enabling the user to complete one-click deployment of the generated services.\\n*   The subscriber authorizes their VPC rights to the software vendor either temporarily or permanently through the authorization capabilities provided by the cloud vendor, so that the software vendor can intervene to perform DevOps operations.\\n*   Currently Azure requires the user to take the initiative to submit a work order to the software vendor on the cloud vendor platform to request DevOps. The prerequisite requirement here is that the user is required to identify the problem, and then the software vendor is required to resolve it.\\n*   Since the authorized and operated machine resources come from the cloud vendor platform, Azure is currently providing audit logs specific to the resource level to facilitate users to review the actions done by the software vendor on their machines when needed, but service level auditing is not yet available.\\n\\nFrom a personal point of view, the current form of Managed Service provided by Azure is still quite a distance from the ideal state of managed service model.\\n\\n*   Cloud vendors need to provide better control and oversight roles, and provide a more fine-grained level of service audit logging capabilities.\\n*   The discovery of DevOps issues should ideally not depend on the user, which would be similar to the SaaS fully managed experience, so that the user does not need to care about DevOps and does not need to have knowledge about it.\\n![](http://www-cdn.nebula-graph.io/nebula-website-5.0/images/blogs/azure-marketplace-three-cloud-service-models-for-isvs/figure3.png)\\n\\n## Conclusion\\n\\nThis is part of Nebula Graph's past experience in exploring cloud-based graph database services. If we compare the three service models, from my personal point of view, the fully managed cloud service model is undoubtedly the best choice for the future as people's usage habits change and the corresponding infrastructure is improved. We have recently released our SaaS [fully managed offering of Nebula Graph](https://portal.azure.com/#create/vesoftcompanylimited1625556994617.nebula_graph_cloud/preview) on the Azure Marketplace and you are welcome to try it out.\\n\\nIn addition, for users who need to keep their data in their own hands, our Manage Service on Azure is also under development, please stay tuned.\\n\\n## About the author\\n\\nJerry Liang is a technical lead at Nebula Graph’s Cloud division. He and his team are behind a series of Nebula Graph’s visualization tools such as [Nebula Dashboard](https://nebula-graph.io/products/dashboard/), [Nebula Explorer](https://nebula-graph.io/products/explorer/), and [Nebula Studio](https://nebula-graph.io/products/studio/).\\n\\n*   [Part I: How to list your cloud offer successfully?](https://nebula-graph.io/posts/how-to-list-cloud-offer-azure-marketplace)\\n*   [Part II: A deep dive into Azure's billing system](https://nebula-graph.io/posts/a-deep-dive-into-azure-marketplace-billing-system)\\n*   [Part III: My takes on three cloud service models for ISVs](http://nebula-graph.io/posts/azure-marketplace-three-cloud-service-models-for-isvs)\",\n            \"description\": \"In addition to SaaS services, Azure Marketplace has other service models. In this article I will talk about the difference between Azure's SaaS Fully managed service, Azure Application Self-service provisioning, and its Managed Service.\",\n            \"pic\": \"https://www-cdn.nebula-graph.io/nebula-website-5.0/images/blogs/azure-marketplace-three-cloud-service-models-for-isvs/banner.jpg\",\n            \"publish\": false,\n            \"slug\": \"azure-marketplace-three-cloud-service-models-for-isvs\",\n            \"title\": \"Azure Marketplace series: My take on three cloud service models for ISVs\",\n            \"createUserId\": 1,\n            \"updateUserId\": 1,\n            \"blogCategoryId\": 1,\n            \"categoryName\": \"Blog\",\n            \"tags\": []\n        }, {\n            \"id\": 300,\n            \"createTime\": \"2022-06-14T14:26:00\",\n            \"updateTime\": \"2022-06-14T14:26:00\",\n            \"author\": \"Wey Gu\",\n            \"content\": \"> Do I have to create my own graph model and everything to set up a Data Lineage system? Thanks to many great open-source projects, the answer is: No!\\n>\\n> Today, I would like to share my opinionated reference data infra stack with some of those best open-source projects with modern ETL, Dashboard, Metadata Governance, and Data Lineage Management.\\n\\n<!--more-->\\n\\n## Metadata Governance system\\n\\nA Metadata Governance system is a system providing a single view of where and how all the data are formatted, generated, transformed, consumed, presented, and owned.\\n\\nMetadata Governance is like a catalog of all of the data warehouses, databases, tables, dashboards, ETL jobs, etc so that people don't have to broadcast their queries on \\\"Hi everyone, could I change the schema of this table?\\\", \\\"Hey, anyone who knows how I could find the raw data of table-view-foo-bar?\\\", which, explains why we need a Metadata Governance system in a mature data stack with a relatively large scale of data and team(or one to be grown to).\\n\\nFor the other term, Data Lineage, is one of the Metadata that needs to be managed, for example, some dashboard is the downstream of a table view, which has an upstream as two other tables from different databases. That information should be managed at best when possible, too, to enable a trust chain on a data-driven team.\\n\\n## The reference solution\\n\\n### Motivation\\n\\nThe metadata and data lineage are by nature fitting to the graph model/graph database well, and the relationship-oriented queries, for instance, \\\"finding all n-depth data lineage per given component(i.e. a table)\\\" is a `FIND ALL PATH` query in a graph database.\\n\\nThis also explains one observation of mine as an OSS contributor of Nebula Graph, a distributed graph database: (from their queries/graph modeling in discussions I could tell) a bunch of teams who are already levering Nebula Graph on their tech stack, are setting up a data lineage system on their own, from scratch.\\n\\nA Metadata Governance system needs some of the following components:\\n\\n- Metadata Extractor\\n  - This part is needed to either pull or be pushed from the different parties of the data stack like databases, data warehouses, dashboards, or even from ETL pipeline and applications, etc.\\n- Metadata Storage\\n  - This could be either a database or even large JSON manifest files\\n- Metadata Catalog\\n  - This could be a system providing API and/or a GUI interface to read/write the metadata and data lineage\\n\\nIn Nebula Graph community, I had been seeing many graph database users were building their in-house data lineage system. It’s itching witnessing this entropy increase situation not be standarized or jointly contributed instead, as most of their work are parsing metadata from well-known big-data projects, and persistent into a graph database, which, I consider high probability that the work is common.\\n\\nThen I came to create an opinionated reference data infra stack with some of those best open-source projects put together. Hopefully, those who were gonna define and iterate their own fashion of Graph Model on Nebula Graph and create in-house Metadata and data linage extracting pipelines can benefit from this project to have a relatively polished, beautifully designed, Metadata Governance system out of the box with a fully evolved graph model.\\n\\nTo make the reference project self-contained and runnable, I tried to put layers of data infra stack more than just pure metadata related ones, thus, maybe it will help new data engineers who would like to try and see how far had open-source pushed a modern data lab to.\\n\\nThis is a diagram of all the components in this reference data stack, where I see most of them as Metadata Sources:\\n\\n![diagram-of-ref-project](https://user-images.githubusercontent.com/1651790/168849779-4826f50e-ff87-4e78-b17f-076f91182c43.svg)\\n\\n### The Data Stack\\n\\nThen, let's introduce the components.\\n\\n#### Database and Data Warehouse\\n\\nFor processing and consuming raw and intermediate data, one or more databases and/or warehouses should be used.\\n\\nIt could be any DB/DW like Hive, Apache Delta, TiDB, Cassandra, MySQL, or Postgres, in this reference project, we simply choose one of the most popular ones: Postgres. And our reference lab comes with the first service:\\n\\n✅ - Data warehouse: Postgres\\n\\n#### DataOps\\n\\nWe should have some sort of DataOps setup to enable pipelines and environments to be repeatable, testable, and version-controlled.\\n\\nHere, we used [Meltano](https://gitlab.com/meltano/meltano) created by GitLab.\\n\\nMeltano is a just-work DataOps platform that connected [Singer](https://singer.io/) as the EL and [dbt](https://getdbt.com/) as the T in a magically elegant way, it is also connected to some other dataInfra utilities such as Apache Superset and Apache Airflow, etc.\\n\\nThus, we have one more thing to be included:\\n\\n✅ - GitOps: Meltano\\n\\n#### ETL\\n\\nAnd under the hood, we will E(extract) and L(load) data from many different data sources to data targets leveraging [Singer](https://singer.io/) together with Meltano, and do T(transformation) with [dbt](https://getdbt.com/).\\n\\n✅ - EL: Singer\\n\\n✅ - T: dbt\\n\\n#### Data Visualization\\n\\nHow about creating dashboards, charts, and tables for getting the insights into all the data?\\n\\n![](https://user-images.githubusercontent.com/1651790/172800854-8e01acae-696d-4e07-8e3e-a7d34dec8278.png)\\n\\n[Apache Superset](https://superset.apache.org/) is one of the greatest visualization platforms we could choose from, and we just add it to our packet!\\n\\n✅ - Dashboard: Apache Superset\\n\\n#### Job Orchestration\\n\\nIn most cases, our DataOps jobs grow to the scale to be executed in a long time that needs to be orchestrated, and here comes the [Apache Airflow](https://airflow.apache.org/).\\n\\n✅ - DAG: Apache Airflow\\n\\n#### Metadata governance\\n\\nWith more components and data being introduced to a data infra, there will be massive metadata in all lifecycle of databases, tables, schemas, dashboards, DAGs, applications, and their administrators and teams could be collectively managed, connected, and discovered.\\n\\n[Linux Foundation Amundsen](https://www.amundsen.io/amundsen/) is one of the best projects solving this problem.\\n\\n![](https://user-images.githubusercontent.com/1651790/172801018-ecd67fa9-2743-451f-8734-b14f2a814199.png)\\n\\n\\n\\n✅ - Data Discovery: Linux Foundation Amundsen\\n\\nWith a graph database as the source of truth to accelerate the multi-hop queries together with elasticsearch as the full-text search engine, Amundsen indexes all the metadata and their lineage smoothly, and beautifully in the next level.\\n\\nBy default, [neo4j](https://neo4j.org/) was used as the graph database, while I will be using [Nebula Graph](http://nebula-graph.io/) instead in this project due to I am more familiar with the latter.\\n\\n✅ - Full-text Search: elasticsearch\\n\\n✅ - Graph Database: Nebula Graph\\n\\nNow, with the components in our stack being revealed, let's have them assembled.\\n\\n## Environment Bootstrap, Component overview\\n\\nThe reference runnable project is open-source and you could find it here:\\n\\n- https://github.com/wey-gu/data-lineage-ref-solution\\n\\nI will try my best to make things clean and isolated. It's assumed you are running on a UNIX-like system with internet and Docker Compose being installed.\\n\\n> Please refer [here](https://docs.docker.com/compose/install/) to install Docker and Docker Compose before moving forward.\\n\\nI am running it on Ubuntu 20.04 LTS X86_64, but there shouldn't be issues on other distros or versions of Linux.\\n\\n### Run a Data Warehouse/ Database\\n\\nFirst, let's install Postgres as our data warehouse.\\n\\nThis oneliner will help create a Postgres running in the background with docker, and when being stopped it will be cleaned up(`--rm`).\\n\\n```bash\\ndocker run --rm --name postgres \\\\\\n    -e POSTGRES_PASSWORD=lineage_ref \\\\\\n    -e POSTGRES_USER=lineage_ref \\\\\\n    -e POSTGRES_DB=warehouse -d \\\\\\n    -p 5432:5432 postgres\\n```\\n\\nThen we could verify it with Postgres CLI or GUI clients.\\n\\n> Hint: You could use VS Code extension: [SQL tools](https://marketplace.visualstudio.com/items?itemName=mtxr.sqltools) to quickly connect to multiple RDBMS(MariaDB, Postgres, etc.) or even Non-SQL DBMS like Cassandra in a GUI fashion.\\n\\n### Setup DataOps toolchain for ETL\\n\\nThen, let's get Meltano with Singler and dbt installed.\\n\\nMeltano helps us manage ETL utilities(as plugins) and all of their configurations(the pipelines). Those meta-information sits in meltano configurations and its [system database](https://docs.meltano.com/concepts/project#system-database), where the configurations are file-based(could be managed with git) and by default the system database is SQLite.\\n\\n#### Installation of Meltano\\n\\nThe workflow using Meltano is to initiate a `meltano project` and start to add E, L, and T into the configuration files. The initiation of a project just requires a CLI command call: `meltano init yourprojectname` and to do that, we could install Meltano either with Python's package manager: pip or via a Docker image:\\n\\n- Install Meltano with pip in a python virtual env:\\n\\n```bash\\nmkdir .venv\\n# example in a debian flavor Linux distro\\nsudo apt-get install python3-dev python3-pip python3-venv python3-wheel -y\\npython3 -m venv .venv/meltano\\nsource .venv/meltano/bin/activate\\npython3 -m pip install wheel\\npython3 -m pip install meltano\\n\\n# init a project\\nmkdir meltano_projects && cd meltano_projects\\n# replace <yourprojectname> with your own one\\ntouch .env\\nmeltano init <yourprojectname>\\n```\\n\\n- \\\"Install\\\" Meltano via Docker\\n\\n```bash\\ndocker pull meltano/meltano:latest\\ndocker run --rm meltano/meltano --version\\n\\n# init a project\\nmkdir meltano_projects && cd meltano_projects\\n\\n# replace <yourprojectname> with your own one\\ntouch .env\\ndocker run --rm -v \\\"$(pwd)\\\":/projects \\\\\\n             -w /projects --env-file .env \\\\\\n             meltano/meltano init <yourprojectname>\\n```\\n\\nApart from `meltano init`, there are a couple of other commands like `meltano etl` to perform ETL executions, and `meltano invoke <plugin>` to call plugins' command, always check the [cheatsheet](https://docs.meltano.com/reference/command-line-interface) for quick referencing.\\n\\n#### The Meltano UI\\n\\nMeltano also comes with a web-based UI, to start it, just run:\\n\\n```bash\\nmeltano ui\\n```\\n\\nThen it's listening to http://localhost:5000.\\n\\nFor Docker, just run the container with the 5000 port exposed, here we didn't provide `ui` in the end due to the container's default command being `meltano ui` already.\\n\\n```bash\\ndocker run -v \\\"$(pwd)\\\":/project \\\\\\n             -w /project \\\\\\n             -p 5000:5000 \\\\\\n             meltano/meltano\\n```\\n\\n#### Example Meltano projects\\n\\nWhen writing this article, I noticed that [Pat Nadolny](https://github.com/pnadolny13) had created [great examples](https://github.com/pnadolny13/meltano_example_implementations/tree/main/meltano_projects/singer_dbt_jaffle) on an example dataset for Meltano with dbt(And with [Airflow](https://github.com/pnadolny13/meltano_example_implementations/tree/main/meltano_projects/dbt_orchestration) and [Superset](https://github.com/pnadolny13/meltano_example_implementations/tree/main/meltano_projects/jaffle_superset), too!). We will not recreate the examples and use Pat's great ones.\\n\\n> Note that Andrew Stewart had created another one with a slightly older version of configuration files.\\n\\nYou could follow [here](https://github.com/pnadolny13/meltano_example_implementations/tree/main/meltano_projects/singer_dbt_jaffle) to run a pipeline of:\\n\\n- [tap-CSV](https://hub.meltano.com/taps/csv)(Singer), extracting data from CSV files\\n- [target-postgres](https://hub.meltano.com/targets/postgres)(Singer), loading data to Postgres\\n- [dbt](https://hub.meltano.com/transformers/dbt), transform the data into aggregated tables or views\\n\\n> You should omit the step of running the local Postgres with docker as we had already created one, be sure to change the Postgres user and password in `.env`.\\n>\\n> And it's basically as this(with meltano being installed as above):\\n>\\n> ```bash\\n> git clone https://github.com/pnadolny13/meltano_example_implementations.git\\n> cd meltano_example_implementations/meltano_projects/singer_dbt_jaffle/\\n> \\n> meltano install\\n> touch .env\\n> echo PG_PASSWORD=\\\"lineage_ref\\\" >> .env\\n> echo PG_USERNAME=\\\"lineage_ref\\\" >> .env\\n> \\n> # Extract and Load(with Singer)\\n> meltano run tap-csv target-postgres\\n> \\n> # Trasnform(with dbt)\\n> meltano run dbt:run\\n> \\n> # Generate dbt docs\\n> meltano invoke dbt docs generate\\n> \\n> # Serve generated dbt docs\\n> meltano invoke dbt docs to serve\\n> \\n> # Then visit http://localhost:8080\\n> ```\\n\\nNow, I assumed you had finished trying out `singer_dbt_jaffle` following its [README.md](https://github.com/pnadolny13/meltano_example_implementations/tree/main/meltano_projects/singer_dbt_jaffle), and we could connect to the Postgres to see the loaded and transformed data being reflected as follow, the screenshot is from the SQLTool of VS Code:\\n\\n![](https://user-images.githubusercontent.com/1651790/167540494-01e3dbd2-6ab1-41d2-998e-3b79f755bdc7.png)\\n\\n### Setup a BI Platform for Dashboard\\n\\nNow, we have the data in data warehouses, with ETL toolchains to pipe different data sources into it. How could those data be consumed?\\n\\nBI tools like the dashboard could be one way to help us get insights from the data.\\n\\nWith Apache Superset, dashboards, and charts based on those data sources could be created and managed smoothly and beautifully.\\n\\nThe focus of this project was not on Apache Superset itself, thus, we simply reuse examples that [Pat Nadolny](https://github.com/pnadolny13) had created in [Superset as a utility if meltano Example](https://github.com/pnadolny13/meltano_example_implementations/tree/main/meltano_projects/jaffle_superset).\\n\\n#### Bootstrap Meltano and Superset\\n\\nCreate a python venv with Meltano installed:\\n\\n```bash\\nmkdir .venv\\npython3 -m venv .venv/meltano\\nsource .venv/meltano/bin/activate\\npython3 -m pip install wheel\\npython3 -m pip install meltano\\n```\\n\\nFollowing [Pat's guide](https://github.com/pnadolny13/meltano_example_implementations/tree/main/meltano_projects/jaffle_superset), with tiny modifications:\\n\\n- Clone the repo, enter the `jaffle_superset` project\\n\\n```bash\\ngit clone https://github.com/pnadolny13/meltano_example_implementations.git\\ncd meltano_example_implementations/meltano_projects/jaffle_superset/\\n```\\n\\n- Modify the meltano configuration files to let Superset connect to the Postgres we created:\\n\\n```bash\\nvim meltano_projects/jaffle_superset/meltano.yml\\n```\\n\\nIn my example, I changed the hostname to `10.1.1.111`, which is the IP of my current host, while if you are running it on your macOS machine, this should be fine to leave with it, the diff before and after the change would be:\\n\\n```diff\\n--- a/meltano_projects/jaffle_superset/meltano.yml\\n+++ b/meltano_projects/jaffle_superset/meltano.yml\\n@@ -71,7 +71,7 @@ plugins:\\n               A list of database driver dependencies can be found here https://superset.apache.org/docs/databases/installing-database-drivers\\n     config:\\n       database_name: my_postgres\\n-      sqlalchemy_uri: postgresql+psycopg2://${PG_USERNAME}:${PG_PASSWORD}@host.docker.internal:${PG_PORT}/${PG_DATABASE}\\n+      sqlalchemy_uri: postgresql+psycopg2://${PG_USERNAME}:${PG_PASSWORD}@10.1.1.168:${PG_PORT}/${PG_DATABASE}\\n       tables:\\n       - model.my_meltano_project.customers\\n       - model.my_meltano_project.orders\\n```\\n\\n- Add Postgres credential to `.env` file:\\n\\n```bash\\necho PG_USERNAME=lineage_ref >> .env\\necho PG_PASSWORD=lineage_ref >> .env\\n```\\n\\n- Install the Meltano project, run ETL pipeline\\n\\n```bash\\nmeltano install\\nmeltano run tap-csv target-postgres dbt:run\\n```\\n\\n- Start Superset, please note that the `ui` is not a meltano command but a user-defined action in the configuration file.\\n\\n```bash\\nmeltano invoke superset:ui\\n```\\n\\n- In another terminal, run the defined command `load_datasources`\\n\\n```\\nmeltano invoke superset:load_datasources\\n```\\n\\n- Access Superset in a web browser via http://localhost:8088/\\n\\nWe should now see Superset Web Interface:\\n\\n![](https://user-images.githubusercontent.com/1651790/168570300-186b56a5-58e8-4ff1-bc06-89fd77d74166.png)\\n\\n#### Create a Dashboard!\\n\\nLet's try to create a Dashboard on the ETL data in Postgres defined in this Meltano project:\\n\\n- Click `+ DASHBOARD`, fill a dashboard name, then click `SAVE`, then clieck `+ CREATE A NEW CHART`\\n\\n![](https://user-images.githubusercontent.com/1651790/168570363-c6b4f929-2aad-4f03-8e3e-b1b61f560ce5.png)\\n\\n- In new chart view, we should select a chart type and DATASET. Here, I selected `orders` table as the data source and `Pie Chart` chart type:\\n\\n![](https://user-images.githubusercontent.com/1651790/168570927-9559a2a1-fed7-43be-9f6a-f6fb3c263830.png)\\n\\n- After clicking `CREATE NEW CHART`, we are in the chart defination view, where, I selected `Query` of `status` as `DIMENSIONS`, and `COUNT(amount)` as `METRIC`. Thus, we could see a Pie Chart per order status's distribution.\\n\\n![](https://user-images.githubusercontent.com/1651790/168571130-a65ba88e-1ebe-4699-8783-08e5ecf54a0c.png)\\n\\n- Click `SAVE` , it will ask which dashboard this chart should be added to, after it's selected, click `SAVE & GO TO DASHBOARD`.\\n\\n![](https://user-images.githubusercontent.com/1651790/168571301-8ae69983-eda8-4e75-99cf-6904f583fc7c.png)\\n\\n- Then, in the dashboard, we coulds see all charts there. You could see that I added another chart showing customer order count distribution, too:\\n\\n![](https://user-images.githubusercontent.com/1651790/168571878-30a77057-1f66-448a-9bbd-0dedcee24cc9.png)\\n\\n- We could set the refresh inteval, or download the dashboard as you wish by clicking the `···` button.\\n\\n![](https://user-images.githubusercontent.com/1651790/168573874-b5d57919-2866-4b3c-a4e5-55b6e6ef342e.png)\\n\\nIt's quite cool, ah? For now, we have a simple but typical data stack like any hobby data lab with everything open-source!\\n\\nImagine we have 100 datasets in CSV, 200 tables in Data warehouse and a couple of data engineers running different projects that consume, generate different application, dashboard, and databases. When someone would like to discovery some of those table, dataset, dashboard and pipelines running across them, and then even modify some of them, it's proven to be quite costly in both communicationand engineering.\\n\\nHere comes the main part of our reference project: Metadata Discovery.\\n\\n### Metadata Discovery\\n\\nThen, we are stepping to deploy the Amundsen with Nebula Graph and Elasticsearch.\\n\\n> Note: For the time being, the [PR Nebula Graph as the Amundsen backend](https://github.com/amundsen-io/amundsen/pull/1817) is not yet merged, I am [working with the Amundsen team](https://github.com/amundsen-io/rfcs/pull/48) to make it happen.\\n\\nWith Amundsen, we could have all metadata of the whole data stack being discovered and managed in one place. And there are mainly two parts of Amundsen:\\n\\n- Metadata Ingestion\\n  - [Amundsen Data builder](https://www.amundsen.io/amundsen/databuilder/)\\n- Metadata Catalog\\n  - [Amundsen Frontend service](https://www.amundsen.io/amundsen/frontend/)\\n  - [Amundsen Metadata service](https://www.amundsen.io/amundsen/metadata/)\\n  - [Amundsen Search service](https://www.amundsen.io/amundsen/search/)\\n\\nWe will be leveraging `Data builder` to pull metadata from different sources, and persist metadata into the backend storage of the `Meta service` and the backend storage of the `Search service`, then we could search, discover and manage them from the `Frontend service` or through the API of the `Metadata service`.\\n\\n#### Deploy Amundsen\\n\\n##### Metadata service\\n\\nWe are going to deploy a cluster of Amundsen with its docker-compose file. As the Nebula Graph backend support is not yet merged, we are referring to my fork.\\n\\nFirst, let's clone the repo with all submodules:\\n\\n```bash\\ngit clone -b amundsen_nebula_graph --recursive git@github.com:wey-gu/amundsen.git\\ncd amundsen\\n```\\n\\nThen, start all catalog services and their backend storage:\\n\\n```bash\\ndocker-compose -f docker-amundsen-nebula.yml up\\n```\\n\\n> You could add `-d` to put the containers running in the background:\\n>\\n> ```bash\\n> docker-compose -f docker-amundsen-nebula.yml up -d\\n> ```\\n>\\n> And this will stop the cluster:\\n>\\n> ```bash\\n> docker-compose -f docker-amundsen-nebula.yml stop\\n> ```\\n>\\n> This will remove the cluster:\\n>\\n> ```bash\\n> docker-compose -f docker-amundsen-nebula.yml down\\n> ```\\n\\nDue to this docker-compose file is for developers to play and hack Amundsen easily rather than for production deployment, it's building images from the codebase, which, will take some time for the very first time.\\n\\nAfter it's being deployed, please hold on a second before we load some dummy data into its storage with Data builder.\\n\\n##### Data builder\\n\\nAmundsen Data builder is just like a Meltano but for ETL of Metadata to `Metadata service` and `Search service`‘s backend storage: Nebula Graph and Elasticsearch. The Data builder here is only a python module and the ETL job could be either run as a script or orchestrated with a DAG platform like Apache Airflow.\\n\\nWith [Amundsen Data builder](https://github.com/amundsen-io/amundsen/tree/main/databuilder) being installed:\\n\\n```bash\\ncd databuilder\\npython3 -m venv .venv\\nsource .venv/bin/activate\\npython3 -m pip install wheel\\npython3 -m pip install -r requirements.txt\\npython3 setup.py install\\n```\\n\\nLet's call this sample Data builder ETL script to have some dummy data filled in.\\n\\n```bash\\npython3 example/scripts/sample_data_loader_nebula.py\\n```\\n\\n##### Verify Amundsen\\n\\nBefore accessing Amundsen, we need to create a test user:\\n\\n```bash\\n# run a container with curl attached to amundsenfrontend\\ndocker run -it --rm --net container:amundsenfrontend nicolaka/netshoot\\n\\n# Create a user with id test_user_id\\ncurl -X PUT -v http://amundsenmetadata:5002/user \\\\\\n    -H \\\"Content-Type: application/json\\\" \\\\\\n    --data \\\\\\n    '{\\\"user_id\\\":\\\"test_user_id\\\",\\\"first_name\\\":\\\"test\\\",\\\"last_name\\\":\\\"user\\\", \\\"email\\\":\\\"test_user_id@mail.com\\\"}'\\n\\nexit\\n```\\n\\nThen we could view UI at [`http://localhost:5000`](http://localhost:5000/) and try to search `test`, it should return some results.\\n\\n![](https://github.com/amundsen-io/amundsen/raw/master/docs/img/search-page.png)\\n\\nThen you could click and explore those dummy metadata loaded to Amundsen during the `sample_data_loader_nebula.py` on your own.\\n\\nAdditionally, you could access the Graph Database with Nebula Studio(http://localhost:7001).\\n\\n> Note in Nebula Studio, the default fields to log in will be:\\n>\\n> - Hosts: `graphd:9669`\\n> - User: `root`\\n> - Password: `nebula`\\n\\nThis diagram shows some more details on the components of Amundsen:\\n\\n```asciiarmor\\n       ┌────────────────────────┐ ┌────────────────────────────────────────┐\\n       │ Frontend:5000          │ │ Metadata Sources                       │\\n       ├────────────────────────┤ │ ┌────────┐ ┌─────────┐ ┌─────────────┐ │\\n       │ Metaservice:5001       │ │ │        │ │         │ │             │ │\\n       │ ┌──────────────┐       │ │ │ Foo DB │ │ Bar App │ │ X Dashboard │ │\\n  ┌────┼─┤ Nebula Proxy │       │ │ │        │ │         │ │             │ │\\n  │    │ └──────────────┘       │ │ │        │ │         │ │             │ │\\n  │    ├────────────────────────┤ │ └────────┘ └─────┬───┘ └─────────────┘ │\\n┌─┼────┤ Search searvice:5002   │ │                  │                     │\\n│ │    └────────────────────────┘ └──────────────────┼─────────────────────┘\\n│ │    ┌─────────────────────────────────────────────┼───────────────────────┐\\n│ │    │                                             │                       │\\n│ │    │ Databuilder     ┌───────────────────────────┘                       │\\n│ │    │                 │                                                   │\\n│ │    │ ┌───────────────▼────────────────┐ ┌──────────────────────────────┐ │\\n│ │ ┌──┼─► Extractor of Sources           ├─► nebula_search_data_extractor │ │\\n│ │ │  │ └───────────────┬────────────────┘ └──────────────┬───────────────┘ │\\n│ │ │  │ ┌───────────────▼────────────────┐ ┌──────────────▼───────────────┐ │\\n│ │ │  │ │ Loader filesystem_csv_nebula   │ │ Loader Elastic FS loader     │ │\\n│ │ │  │ └───────────────┬────────────────┘ └──────────────┬───────────────┘ │\\n│ │ │  │ ┌───────────────▼────────────────┐ ┌──────────────▼───────────────┐ │\\n│ │ │  │ │ Publisher nebula_csv_publisher │ │ Publisher Elasticsearch      │ │\\n│ │ │  │ └───────────────┬────────────────┘ └──────────────┬───────────────┘ │\\n│ │ │  └─────────────────┼─────────────────────────────────┼─────────────────┘\\n│ │ └────────────────┐   │                                 │\\n│ │    ┌─────────────┼───►─────────────────────────┐ ┌─────▼─────┐\\n│ │    │ Nebula Graph│   │                         │ │           │\\n│ └────┼─────┬───────┴───┼───────────┐     ┌─────┐ │ │           │\\n│      │     │           │           │     │MetaD│ │ │           │\\n│      │ ┌───▼──┐    ┌───▼──┐    ┌───▼──┐  └─────┘ │ │           │\\n│ ┌────┼─►GraphD│    │GraphD│    │GraphD│          │ │           │\\n│ │    │ └──────┘    └──────┘    └──────┘  ┌─────┐ │ │           │\\n│ │    │ :9669                             │MetaD│ │ │  Elastic  │\\n│ │    │ ┌────────┐ ┌────────┐ ┌────────┐  └─────┘ │ │  Search   │\\n│ │    │ │        │ │        │ │        │          │ │  Cluster  │\\n│ │    │ │StorageD│ │StorageD│ │StorageD│  ┌─────┐ │ │  :9200    │\\n│ │    │ │        │ │        │ │        │  │MetaD│ │ │           │\\n│ │    │ └────────┘ └────────┘ └────────┘  └─────┘ │ │           │\\n│ │    ├───────────────────────────────────────────┤ │           │\\n│ └────┤ Nebula Studio:7001                        │ │           │\\n│      └───────────────────────────────────────────┘ └─────▲─────┘\\n└──────────────────────────────────────────────────────────┘\\n```\\n\\n\\n\\n\\n\\n## Connecting the dots, Metadata Discovery\\n\\nWith the basic environment being set up, let's put everything together.\\n\\nRemember we had ELT some data to PostgreSQL as this?\\n\\n![](https://user-images.githubusercontent.com/1651790/167540494-01e3dbd2-6ab1-41d2-998e-3b79f755bdc7.png)\\n\\nHow could we let Amundsen discover metadata regarding those data and ETL?\\n\\n### Extracting Postgres metadata\\n\\nWe started on the data source: Postgres, first.\\n\\nWe install the Postgres Client for python3:\\n\\n```bash\\nsudo apt-get install libpq-dev\\npip3 install Psycopg2\\n```\\n\\n#### Execution of Postgres metadata ETL\\n\\nRun a script to parse Postgres Metadata:\\n\\n```bash\\nexport CREDENTIALS_POSTGRES_USER=lineage_ref\\nexport CREDENTIALS_POSTGRES_PASSWORD=lineage_ref\\nexport CREDENTIALS_POSTGRES_DATABASE=warehouse\\n\\npython3 example/scripts/sample_postgres_loader_nebula.py\\n```\\n\\nIf you look into the code of the sample script for loading Postgres metadata to Nebula, the main lines are quite straightforward:\\n\\n```python\\n# part 1: PostgressMetadata --> CSV --> Nebula Graph\\njob = DefaultJob(\\n      conf=job_config,\\n      task=DefaultTask(\\n          extractor=PostgresMetadataExtractor(),\\n          loader=FsNebulaCSVLoader()),\\n      publisher=NebulaCsvPublisher())\\n\\n...\\n# part 2: Metadata stored in NebulaGraph --> Elasticsearch\\nextractor = NebulaSearchDataExtractor()\\ntask = SearchMetadatatoElasticasearchTask(extractor=extractor)\\n\\njob = DefaultJob(conf=job_config, task=task)\\n```\\n\\nThe first job was to load data in path:`PostgressMetadata --> CSV --> Nebula Graph`\\n\\n- `PostgresMetadataExtractor` was used to extract/pull metadata from Postgres, refer [here](https://www.amundsen.io/amundsen/databuilder/#postgresmetadataextractor) for its documentation.\\n- `FsNebulaCSVLoader` was used to put extracted data intermediately as CSV files\\n- `NebulaCsvPublisher` was used to publish metadata in form of CSV to Nebula Graph\\n\\nThe second job was to load in the path: `Metadata stored in NebulaGraph --> Elasticsearch` \\n\\n- `NebulaSearchDataExtractor` was used to fetch metadata stored in Nebula Graph\\n- `SearchMetadatatoElasticasearchTask` was used to make metadata indexed with Elasticsearch.\\n\\n> Note, in production, we could trigger those jobs either in scripts or with an orchestration platform like Apache Airflow.\\n\\n#### Verify the Postgres Extraction\\n\\nSearch `payments` or directly visit http://localhost:5000/table_detail/warehouse/postgres/public/payments, you could see the metadata from our Postgres like:\\n\\n![](https://user-images.githubusercontent.com/1651790/168475180-ebfaa188-268c-4fbe-a614-135d56d07e5d.png)\\n\\nThen, metadata management actions like adding tags, owners, and descriptions could be done easily as it was in the above screen capture, too.\\n\\n### Extracting dbt metadata\\n\\nActually, we could also pull metadata from [dbt](https://www.getdbt.com/) itself.\\n\\nThe Amundsen [DbtExtractor](https://www.amundsen.io/amundsen/databuilder/#dbtextractor), will parse the `catalog.json` or `manifest.json` file to load metadata to Amundsen storage(Nebula Graph and Elasticsearch).\\n\\nIn above meltano chapter, we had already generated that file with `meltano invoke dbt docs generate`, and the output like the following is telling us the `catalog.json` file:\\n\\n```log\\n14:23:15  Done.\\n14:23:15  Building catalog\\n14:23:15  Catalog written to /home/ubuntu/ref-data-lineage/meltano_example_implementations/meltano_projects/singer_dbt_jaffle/.meltano/transformers/dbt/target/catalog.json\\n```\\n\\n#### Execution of dbt metadata ETL\\n\\nThere is an example script with a sample dbt output files:\\n\\nThe sample dbt files:\\n\\n```bash\\n$ ls -l example/sample_data/dbt/\\ntotal 184\\n-rw-rw-r-- 1 w w   5320 May 15 07:17 catalog.json\\n-rw-rw-r-- 1 w w 177163 May 15 07:17 manifest.json\\n```\\n\\nWe could load this sample dbt manifest with:\\n\\n```bash\\npython3 example/scripts/sample_dbt_loader_nebula.py\\n```\\n\\nFrom this lines of python code, we could tell those process as:\\n\\n```python\\n# part 1: Dbt manifest --> CSV --> Nebula Graph\\njob = DefaultJob(\\n      conf=job_config,\\n      task=DefaultTask(\\n          extractor=DbtExtractor(),\\n          loader=FsNebulaCSVLoader()),\\n      publisher=NebulaCsvPublisher())\\n\\n...\\n# part 2: Metadata stored in NebulaGraph --> Elasticsearch\\nextractor = NebulaSearchDataExtractor()\\ntask = SearchMetadatatoElasticasearchTask(extractor=extractor)\\n\\njob = DefaultJob(conf=job_config, task=task)\\n```\\n\\nAnd the only differences from the Postgres meta ETL is the `extractor=DbtExtractor()`, where it comes with following confiugrations to get below information regarding dbt projects:\\n\\n- databases_name\\n- catalog_json\\n- manifest_json\\n\\n```python\\njob_config = ConfigFactory.from_dict({\\n  'extractor.dbt.database_name': database_name,\\n  'extractor.dbt.catalog_json': catalog_file_loc,  # File\\n  'extractor.dbt.manifest_json': json.dumps(manifest_data),  # JSON Dumped objecy\\n  'extractor.dbt.source_url': source_url})\\n```\\n\\n#### Verify the dbt Extraction\\n\\nSearch `dbt_demo` or visit http://localhost:5000/table_detail/dbt_demo/snowflake/public/raw_inventory_value to see:\\n\\n![](https://user-images.githubusercontent.com/1651790/168479864-2f73ea73-265f-4cd2-999f-e7effbaf3ec1.png)\\n\\n> Tips: we could optionally enable debug logging to see what had been sent to Elasticsearch and Nebula Graph!\\n>\\n> ```diff\\n> - logging.basicConfig(level=logging.INFO)\\n> + logging.basicConfig(level=logging.DEBUG)\\n> ```\\n\\nOr, alternatively, explore the imported data in Nebula Studio: \\n\\nFirst, click \\\"Start with Vertices\\\", fill in the vertex id: `snowflake://dbt_demo.public/fact_warehouse_inventory`\\n\\n![](https://user-images.githubusercontent.com/1651790/168480047-26c28cde-5df8-40af-8da4-6ab0203094e2.png)\\n\\nThen, we could see the vertex being shown as the pink dot. Let's modify the `Expand` options with:\\n\\n- Direction: Bidirect\\n- Steps: Single with 3\\n\\n![](https://user-images.githubusercontent.com/1651790/168480101-7b7b5824-06d9-4155-87c9-798db0dc7612.png)\\n\\nAnd double click the vertex(dot), it will expand 3 steps in bidirection:\\n\\n![](https://user-images.githubusercontent.com/1651790/168480280-1dc88d1b-1f1e-48fd-9997-972965522ef5.png)\\n\\nFrom this graph view, the insight of the metadata is extremely easy to be explored, right?\\n\\n> Tips, you may like to click the \\uD83D\\uDC41 icon to select some properties to be shown, which was done by me before capturing the screen as above.\\n\\nAnd, what we had seen in the Nebula Studio echoes the data model of Amundsen metadata service, too:\\n\\n![](https://www.amundsen.io/amundsen/img/graph_model.png)\\n\\nFinally, remember we had leveraged dbt to transform some data in meltano, and the menifest file path is `.meltano/transformers/dbt/target/catalog.json`, you can try create a databuilder job to import it.\\n\\n### Extracting Superset metadata\\n\\n[Dashboards](https://www.amundsen.io/amundsen/databuilder/databuilder/extractor/dashboard/apache_superset/apache_superset_metadata_extractor.py), [Charts](https://www.amundsen.io/amundsen/databuilder/databuilder/extractor/dashboard/apache_superset/apache_superset_chart_extractor.py) and the [relationships with Tables](https://www.amundsen.io/amundsen/databuilder/databuilder/extractor/dashboard/apache_superset/apache_superset_table_extractor.py) can be extracted by Amundsen data builder, as we already setup a Superset Dashboard, let's try ingesting its metadata.\\n\\n#### Execution of Superset metadata ETL\\n\\nThe sample superset script will fetch data from Superset and load metadata into Nebula Graph and Elasticsearch.\\n\\n```python\\npython3 sample_superset_data_loader_nebula.py\\n```\\n\\nIf we set the logging level to `DEBUG`, we could actually see lines like:\\n\\n```python\\n# fetching metadata from superset\\nDEBUG:urllib3.connectionpool:http://localhost:8088 \\\"POST /api/v1/security/login HTTP/1.1\\\" 200 280\\nINFO:databuilder.task.task:Running a task\\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:8088\\nDEBUG:urllib3.connectionpool:http://localhost:8088 \\\"GET /api/v1/dashboard?q=(page_size:20,page:0,order_direction:desc) HTTP/1.1\\\" 308 374\\nDEBUG:urllib3.connectionpool:http://localhost:8088 \\\"GET /api/v1/dashboard/?q=(page_size:20,page:0,order_direction:desc) HTTP/1.1\\\" 200 1058\\n...\\n\\n# insert Dashboard\\n\\nDEBUG:databuilder.publisher.nebula_csv_publisher:Query: INSERT VERTEX `Dashboard` (`dashboard_url`, `name`, published_tag, publisher_last_updated_epoch_ms) VALUES  \\\"superset_dashboard://my_cluster.1/3\\\":(\\\"http://localhost:8088/superset/dashboard/3/\\\",\\\"my_dashboard\\\",\\\"unique_tag\\\",timestamp());\\n...\\n\\n# insert a DASHBOARD_WITH_TABLE relationship/edge\\n\\nINFO:databuilder.publisher.nebula_csv_publisher:Importing data in edge files: ['/tmp/amundsen/dashboard/relationships/Dashboard_Table_DASHBOARD_WITH_TABLE.csv']\\nDEBUG:databuilder.publisher.nebula_csv_publisher:Query:\\nINSERT edge `DASHBOARD_WITH_TABLE` (`END_LABEL`, `START_LABEL`, published_tag, publisher_last_updated_epoch_ms) VALUES \\\"superset_dashboard://my_cluster.1/3\\\"->\\\"postgresql+psycopg2://my_cluster.warehouse/orders\\\":(\\\"Table\\\",\\\"Dashboard\\\",\\\"unique_tag\\\", timestamp()), \\\"superset_dashboard://my_cluster.1/3\\\"->\\\"postgresql+psycopg2://my_cluster.warehouse/customers\\\":(\\\"Table\\\",\\\"Dashboard\\\",\\\"unique_tag\\\", timestamp());\\n```\\n\\n#### Verify the Superset Dashboard Extraction\\n\\nBy searching it in Amundsen, we could the Dashboard info now. And we could verify it from Nebula Studio, too.\\n\\n![](https://user-images.githubusercontent.com/1651790/168719624-738323dd-4c6e-475f-a370-f149181c6184.png)\\n\\n> Note, see also the Dashboard's model in Amundsen from [the dashboard ingestion guide](https://www.amundsen.io/amundsen/databuilder/docs/dashboard_ingestion_guide/):\\n>\\n> ![dashboard_graph_modeling](https://www.amundsen.io/amundsen/databuilder/docs/assets/dashboard_graph_modeling.png?raw=true)\\n\\n### Preview data with Superset\\n\\nSuperset could be used to preview Table Data like this. Corresponding documentation could be referred [here](https://www.amundsen.io/amundsen/frontend/docs/configuration/#preview-client), where the API of `/superset/sql_json/` will be called by Amundsen Frontend.\\n\\n![](https://github.com/amundsen-io/amundsenfrontendlibrary/blob/master/docs/img/data_preview.png?raw=true)\\n\\n### Enable Data lineage\\n\\nBy default, data lineage was not enabled, we could enable it by:\\n\\n0. Go to the Amundsen repo, that's also where we run the `docker-compose -f docker-amundsen-nebula.yml up` command\\n\\n```bash\\ncd amundsen\\n```\\n\\n1. Modify frontend  JS configuration:\\n\\n```diff\\n--- a/frontend/amundsen_application/static/js/config/config-default.ts\\n+++ b/frontend/amundsen_application/static/js/config/config-default.ts\\n   tableLineage: {\\n-    inAppListEnabled: false,\\n-    inAppPageEnabled: false,\\n+    inAppListEnabled: true,\\n+    inAppPageEnabled: true,\\n     externalEnabled: false,\\n     iconPath: 'PATH_TO_ICON',\\n     isBeta: false,\\n```\\n\\n2. Now let's run again build for docker image, where the frontend image will be rebuilt.\\n\\n```bash\\ndocker-compose -f docker-amundsen-nebula.yml build\\n```\\n\\nThen, rerun the `up -d` to ensure frontend container to be recreated with new configuration:\\n\\n```bash\\ndocker-compose -f docker-amundsen-nebula.yml up -d\\n```\\n\\nWe could see something like this:\\n\\n```bash\\n$ docker-compose -f docker-amundsen-nebula.yml up -d\\n...\\nRecreating amundsenfrontend           ... done\\n```\\n\\nAfter that, we could visit http://localhost:5000/lineage/table/gold/hive/test_schema/test_table1 to see the `Lineage` is shown as:\\n\\n![](https://user-images.githubusercontent.com/1651790/168838731-79d0e3bc-439e-4f6b-8ef7-83b37e9bcb12.png)\\n\\nWe could click `Downstream`(if there is) to see downstream resources of this table:\\n\\n![](https://user-images.githubusercontent.com/1651790/168839251-efd523af-d729-44cf-a40b-fa83a0852654.png)\\n\\nOr click Lineage to see the graph:\\n\\n![](https://user-images.githubusercontent.com/1651790/168838814-e6ff5152-c24b-470e-a46a-48f183ba7201.png)\\n\\nThere are API for lineage query, too. Here is an example to query that with cURL, where we leverage the netshoot container as we did before for user creation.\\n\\n```bash\\ndocker run -it --rm --net container:amundsenfrontend nicolaka/netshoot\\n\\ncurl \\\"http://amundsenmetadata:5002/table/snowflake://dbt_demo.public/raw_inventory_value/lineage?depth=3&direction=both\\\"\\n```\\n\\nThe above API call was to query linage on both upstream and downstream direction, with depth 3 for table `snowflake://dbt_demo.public/raw_inventory_value`.\\n\\nAnd the result should be like:\\n\\n```json\\n{\\n    \\\"depth\\\": 3,\\n    \\\"downstream_entities\\\": [\\n        {\\n            \\\"level\\\": 2,\\n            \\\"usage\\\": 0,\\n            \\\"key\\\": \\\"snowflake://dbt_demo.public/fact_daily_expenses\\\",\\n            \\\"parent\\\": \\\"snowflake://dbt_demo.public/fact_warehouse_inventory\\\",\\n            \\\"badges\\\": [],\\n            \\\"source\\\": \\\"snowflake\\\"\\n        },\\n        {\\n            \\\"level\\\": 1,\\n            \\\"usage\\\": 0,\\n            \\\"key\\\": \\\"snowflake://dbt_demo.public/fact_warehouse_inventory\\\",\\n            \\\"parent\\\": \\\"snowflake://dbt_demo.public/raw_inventory_value\\\",\\n            \\\"badges\\\": [],\\n            \\\"source\\\": \\\"snowflake\\\"\\n        }\\n    ],\\n    \\\"key\\\": \\\"snowflake://dbt_demo.public/raw_inventory_value\\\",\\n    \\\"direction\\\": \\\"both\\\",\\n    \\\"upstream_entities\\\": []\\n}\\n```\\n\\nIn fact, this lineage data was just extracted and loaded during our [DbtExtractor](https://github.com/amundsen-io/amundsen/blob/main/databuilder/databuilder/extractor/dbt_extractor.py) execution, where `extractor.dbt.{DbtExtractor.EXTRACT_LINEAGE}` by default was `True`, thus lineage metadata were created and loaded to Amundsen.\\n\\n#### Get lineage in Nebula Graph\\n\\nTwo of the advantages to use a Graph Database as Metadata Storage are:\\n\\n- The graph query itself is a flexible DSL for lineage API, for example, this query helps us do the equivalent query of the Amundsen metadata API for fetching lineage:\\n\\n```cypher\\nMATCH p=(t:Table) -[:HAS_UPSTREAM|:HAS_DOWNSTREAM *1..3]->(x)\\nWHERE id(t) == \\\"snowflake://dbt_demo.public/raw_inventory_value\\\" RETURN p\\n```\\n\\n- We could now even query it in Nebula Graph Studio's console, and click `View Subgraphs` to make it rendered in a graph view then.\\n\\n![](https://user-images.githubusercontent.com/1651790/168844882-ca3d0587-7946-4e17-8264-9dc973a44673.png)\\n\\n![](https://user-images.githubusercontent.com/1651790/168845155-b0e7a5ce-3ddf-4cc9-89a3-aaf1bbb0f5ec.png)\\n\\n#### Extract Data Lineage\\n\\n##### Dbt\\n\\nAs mentioned above, [DbtExtractor](https://www.amundsen.io/amundsen/databuilder/#dbtextractor) will extract table level lineage, together with other information defined in the dbt ETL pipeline.\\n\\n##### Open Lineage\\n\\nThe other linage extractor out-of-the-box in Amundsen is [OpenLineageTableLineageExtractor](https://www.amundsen.io/amundsen/databuilder/#openlineagetablelineageextractor).\\n\\n[Open Lineage](https://openlineage.io/) is an open framework to collect lineage data from different sources in one place, which can output linage information as JSON files to be extracted by [OpenLineageTableLineageExtractor](https://www.amundsen.io/amundsen/databuilder/#openlineagetablelineageextractor):\\n\\n```python\\ndict_config = {\\n    # ...\\n    f'extractor.openlineage_tablelineage.{OpenLineageTableLineageExtractor.CLUSTER_NAME}': 'datalab',\\n    f'extractor.openlineage_tablelineage.{OpenLineageTableLineageExtractor.OL_DATASET_NAMESPACE_OVERRIDE}': 'hive_table',\\n    f'extractor.openlineage_tablelineage.{OpenLineageTableLineageExtractor.TABLE_LINEAGE_FILE_LOCATION}': 'input_dir/openlineage_nd.json',\\n}\\n...\\n\\ntask = DefaultTask(\\n    extractor=OpenLineageTableLineageExtractor(),\\n    loader=FsNebulaCSVLoader())\\n```\\n\\n## Recap\\n\\nThe whole idea of Metadata Governance/Discovery is to:\\n\\n- Put all components in the stack as Metadata Sources(from any DB or DW to dbt, Airflow, Openlineage, Superset, etc.)\\n- Run metadata ETL with Databuilder(as a script, or DAG) to store and index with Nebula Graph(or other Graph Database) and Elasticsearch\\n- Consume, manage, and discover metadata from Frontend UI(with Superset for preview) or API\\n- Have more possibilities, flexibility, and insights on Nebula Graph from queries and UI\\n\\n![](https://user-images.githubusercontent.com/1651790/168849779-4826f50e-ff87-4e78-b17f-076f91182c43.svg)\\n\\n### Upstream Projects\\n\\nAll projects used in this reference project are listed below in lexicographic order.\\n\\n- Amundsen\\n- Apache Airflow\\n- Apache Superset\\n- dbt\\n- Elasticsearch\\n- meltano\\n- Nebula Graph\\n- Open Lineage\\n- singer\\n\\n\\n\\n> Feature Image credit to [Phil Hearing](https://unsplash.com/photos/PhnJhjH9Y9s)\\n\",\n            \"description\": \"I would like to share my opinionated reference data infrastructure stack with some of those best open-source projects with modern ETL, Dashboard, Metadata Governance, and Data Lineage Management.\",\n            \"pic\": \"https://www-cdn.nebula-graph.io/nebula-website-5.0/images/blogs/metadata-governance-graph-database-data-lineage/banner.jpg\",\n            \"publish\": false,\n            \"slug\": \"metadata-governance-graph-database-data-lineage\",\n            \"title\": \"Introducing a metadata management system that uses Nebula Graph as graph database\",\n            \"createUserId\": 1,\n            \"updateUserId\": 1,\n            \"blogCategoryId\": 1,\n            \"categoryName\": \"Blog\",\n            \"tags\": []\n        }]\n    },\n    \"timestamp\": 1662376727587,\n    \"executeTime\": 1794\n}",
  "description" : null,
  "requestBodyDefinition" : null,
  "responseBodyDefinition" : {
    "name" : "",
    "value" : "",
    "description" : "",
    "required" : false,
    "dataType" : "Object",
    "type" : null,
    "defaultValue" : null,
    "validateType" : "",
    "error" : "",
    "expression" : "",
    "children" : [ {
      "name" : "code",
      "value" : "0",
      "description" : "",
      "required" : false,
      "dataType" : "Integer",
      "type" : null,
      "defaultValue" : null,
      "validateType" : "",
      "error" : "",
      "expression" : "",
      "children" : [ ]
    }, {
      "name" : "message",
      "value" : "success",
      "description" : "",
      "required" : false,
      "dataType" : "String",
      "type" : null,
      "defaultValue" : null,
      "validateType" : "",
      "error" : "",
      "expression" : "",
      "children" : [ ]
    }, {
      "name" : "data",
      "value" : "",
      "description" : "",
      "required" : false,
      "dataType" : "Object",
      "type" : null,
      "defaultValue" : null,
      "validateType" : "",
      "error" : "",
      "expression" : "",
      "children" : [ {
        "name" : "total",
        "value" : "130",
        "description" : "",
        "required" : false,
        "dataType" : "Integer",
        "type" : null,
        "defaultValue" : null,
        "validateType" : "",
        "error" : "",
        "expression" : "",
        "children" : [ ]
      }, {
        "name" : "list",
        "value" : "",
        "description" : "",
        "required" : false,
        "dataType" : "Array",
        "type" : null,
        "defaultValue" : null,
        "validateType" : "",
        "error" : "",
        "expression" : "",
        "children" : [ {
          "name" : "",
          "value" : "",
          "description" : "",
          "required" : false,
          "dataType" : "Object",
          "type" : null,
          "defaultValue" : null,
          "validateType" : "",
          "error" : "",
          "expression" : "",
          "children" : [ {
            "name" : "id",
            "value" : "309",
            "description" : "",
            "required" : false,
            "dataType" : "Integer",
            "type" : null,
            "defaultValue" : null,
            "validateType" : "",
            "error" : "",
            "expression" : "",
            "children" : [ ]
          }, {
            "name" : "createTime",
            "value" : "2022-08-26T04:49:30",
            "description" : "",
            "required" : false,
            "dataType" : "String",
            "type" : null,
            "defaultValue" : null,
            "validateType" : "",
            "error" : "",
            "expression" : "",
            "children" : [ ]
          }, {
            "name" : "updateTime",
            "value" : "2022-08-26T04:49:30",
            "description" : "",
            "required" : false,
            "dataType" : "String",
            "type" : null,
            "defaultValue" : null,
            "validateType" : "",
            "error" : "",
            "expression" : "",
            "children" : [ ]
          }, {
            "name" : "author",
            "value" : "NebulaGraph",
            "description" : "",
            "required" : false,
            "dataType" : "String",
            "type" : null,
            "defaultValue" : null,
            "validateType" : "",
            "error" : "",
            "expression" : "",
            "children" : [ ]
          }, {
            "name" : "content",
            "value" : "This is a generated dataset with two kinds of vertices and four kinds of edges(relationships):\\n\\nperson can hold a corp in {share} % person can be relative with another person corp can hold another corp in {share} % corp can be a branch of another corp person can be as a role of a corp.\\n\\n\\n![data-modeling-for-graph](https://www-cdn.nebula-graph.io/nebula-website-5.0/images/demo/demo-shareholding/graph-modeling.png)\\n\\n\\nWithin the playground, you can visually explore the shareholding data from select vertices(i.e. “c_132” with the name “Chambers LLC”) by selecting:\\n\\n![data-import](https://www-cdn.nebula-graph.io/nebula-website-5.0/images/demo/demo-shareholding/graph-database-vertices.png)\\n\\n\\nclick this explored vertex dot, then you can explore from select vertices by selecting:\\n\\n* Edge Type\\n* Direction\\n* Steps\\n* Query Limit(Optional)\\n\\nnote, you can click the \\uD83D\\uDC41️ icon to add options to show fields of the graph,\\n\\n![graph-database-canvas](https://www-cdn.nebula-graph.io/nebula-website-5.0/images/demo/demo-shareholding/graph-database-canvas.png)\\n\\n\\nAfter clicking Expand, you will see all queried relations with c_132 the Chambers LLC.\\n\\n![graph-database-visualization](https://www-cdn.nebula-graph.io/nebula-website-5.0/images/demo/demo-shareholding/graph-database-visualization.png)\\n\\nAlternatively, you could query nGQL via console like:\\n\\nGO 1 TO 3 STEPS FROM \\\"c_132\\\" over * BIDIRECT;\\n\\nRead for more of the dataset, refer to [https://github.com/wey-gu/nebula-shareholding-example](https://github.com/wey-gu/nebula-shareholding-example])\\n\\nDownload sample dataset: [shareholding-dataset](https://www.kaggle.com/littlewey/nebula-graph-shareolding-dataset)\\n\\n",
            "description" : "",
            "required" : false,
            "dataType" : "String",
            "type" : null,
            "defaultValue" : null,
            "validateType" : "",
            "error" : "",
            "expression" : "",
            "children" : [ ]
          }, {
            "name" : "description",
            "value" : "NebulaGraph Dashboard is an out-of-the-box visualized cluster management tool. It enables cluster operations such as node management, service management, scaling in/out, and configuration update.",
            "description" : "",
            "required" : false,
            "dataType" : "String",
            "type" : null,
            "defaultValue" : null,
            "validateType" : "",
            "error" : "",
            "expression" : "",
            "children" : [ ]
          }, {
            "name" : "pic",
            "value" : "https://www-cdn.nebula-graph.io/nebula-website-5.0/images/demo/demo-shareholding/shareholding-banner.png",
            "description" : "",
            "required" : false,
            "dataType" : "String",
            "type" : null,
            "defaultValue" : null,
            "validateType" : "",
            "error" : "",
            "expression" : "",
            "children" : [ ]
          }, {
            "name" : "publish",
            "value" : "true",
            "description" : "",
            "required" : false,
            "dataType" : "Boolean",
            "type" : null,
            "defaultValue" : null,
            "validateType" : "",
            "error" : "",
            "expression" : "",
            "children" : [ ]
          }, {
            "name" : "slug",
            "value" : "shared-holding",
            "description" : "",
            "required" : false,
            "dataType" : "String",
            "type" : null,
            "defaultValue" : null,
            "validateType" : "",
            "error" : "",
            "expression" : "",
            "children" : [ ]
          }, {
            "name" : "title",
            "value" : "Use NebulaGraph to explore shareholding networks",
            "description" : "",
            "required" : false,
            "dataType" : "String",
            "type" : null,
            "defaultValue" : null,
            "validateType" : "",
            "error" : "",
            "expression" : "",
            "children" : [ ]
          }, {
            "name" : "createUserId",
            "value" : "8",
            "description" : "",
            "required" : false,
            "dataType" : "Integer",
            "type" : null,
            "defaultValue" : null,
            "validateType" : "",
            "error" : "",
            "expression" : "",
            "children" : [ ]
          }, {
            "name" : "updateUserId",
            "value" : "8",
            "description" : "",
            "required" : false,
            "dataType" : "Integer",
            "type" : null,
            "defaultValue" : null,
            "validateType" : "",
            "error" : "",
            "expression" : "",
            "children" : [ ]
          }, {
            "name" : "blogCategoryId",
            "value" : "2",
            "description" : "",
            "required" : false,
            "dataType" : "Integer",
            "type" : null,
            "defaultValue" : null,
            "validateType" : "",
            "error" : "",
            "expression" : "",
            "children" : [ ]
          }, {
            "name" : "categoryName",
            "value" : "Demo",
            "description" : "",
            "required" : false,
            "dataType" : "String",
            "type" : null,
            "defaultValue" : null,
            "validateType" : "",
            "error" : "",
            "expression" : "",
            "children" : [ ]
          }, {
            "name" : "tags",
            "value" : "",
            "description" : "",
            "required" : false,
            "dataType" : "Array",
            "type" : null,
            "defaultValue" : null,
            "validateType" : "",
            "error" : "",
            "expression" : "",
            "children" : [ ]
          } ]
        } ]
      } ]
    }, {
      "name" : "timestamp",
      "value" : "1662376727587",
      "description" : "",
      "required" : false,
      "dataType" : "Long",
      "type" : null,
      "defaultValue" : null,
      "validateType" : "",
      "error" : "",
      "expression" : "",
      "children" : [ ]
    }, {
      "name" : "executeTime",
      "value" : "1794",
      "description" : "",
      "required" : false,
      "dataType" : "Integer",
      "type" : null,
      "defaultValue" : null,
      "validateType" : "",
      "error" : "",
      "expression" : "",
      "children" : [ ]
    } ]
  }
}
================================
var tagData;

if (tag) {
    tagData = db.one_website_en.camel().selectOne(
    """
    select * from website_blog_tag where name = #{tag}
    """
    )
}

if (tagId) {
    tagData=db.one_website_en.camel().selectOne(
    """
    select * from website_blog_tag where id = #{tagId}
    """
    )
}

var related_blogIds;

if (tagData) {
    related_blogIds = db.one_website_en.camel().select(
        """
        select * from website_blog_tag_join where tag_id = #{tagData.id}
        """
    ).map(i => i.blogId)
}

var blogs;

if (categoryName) {
    blogs = db.one_website_en.camel().page("""
        select website_blog.*, website_blog_category.name as category_name from website_blog
        left join website_blog_category on website_blog.blog_category_id = website_blog_category.id 
        where website_blog_category.name = #{categoryName}
        ?{related_blogIds != null, and website_blog.id in (#{related_blogIds})}
        ?{title != null, and website_blog.title like "%"#{title}"%"}
        ?{author != null, and website_blog.author like "%"#{author}"%"}
        ?{publish != null, and website_blog.publish = #{publish}}
        order by website_blog.publish_time desc 
    """)
} else if (categoryId) {
    blogs = db.one_website_en.camel().page("""
        select website_blog.*, website_blog_category.name as category_name from website_blog
        left join website_blog_category on website_blog.blog_category_id = website_blog_category.id
        where website_blog_category.id = #{categoryId}
        ?{related_blogIds != null, and website_blog.id in (#{related_blogIds})}
        ?{title != null, and website_blog.title like "%"#{title}"%"}
        ?{author != null, and website_blog.author like "%"#{author}"%"}
        ?{publish != null, and website_blog.publish = #{publish}}
        order by website_blog.publish_time desc 
    """)
} else if (related_blogIds) {
    blogs = db.one_website_en.camel().page("""
        select website_blog.*, website_blog_category.name as category_name from website_blog
        left join website_blog_category on website_blog.blog_category_id = website_blog_category.id
        where website_blog.id in (#{related_blogIds})
        ?{title != null, and website_blog.title like "%"#{title}"%"}
        ?{author != null, and website_blog.author like "%"#{author}"%"}
        ?{publish != null, and website_blog.publish = #{publish}}
        order by website_blog.publish_time desc 
    """)
} else if (title) {
        blogs = db.one_website_en.camel().page("""
        select website_blog.*, website_blog_category.name as category_name from website_blog
        left join website_blog_category on website_blog.blog_category_id = website_blog_category.id
        where title like "%"#{title}"%"
        ?{author != null, and website_blog.author like "%"#{author}"%"}
        ?{publish != null, and website_blog.publish = #{publish}}
        order by website_blog.publish_time desc 
    """)
} else if (author) {
    blogs = db.one_website_en.camel().page("""
        select website_blog.*, website_blog_category.name as category_name from website_blog
        left join website_blog_category on website_blog.blog_category_id = website_blog_category.id
        where author like "%"#{author}"%"
        ?{publish != null, and website_blog.publish = #{publish}}
        order by website_blog.publish_time desc 
    """)
}  else if (publish != null) {
    blogs = db.one_website_en.camel().page("""
        select website_blog.*, website_blog_category.name as category_name from website_blog
        left join website_blog_category on website_blog.blog_category_id = website_blog_category.id
        where website_blog.publish = #{publish}
        order by website_blog.publish_time desc 
    """)
} else {
        blogs = db.one_website_en.camel().page("""
        select website_blog.*, website_blog_category.name as category_name from website_blog
        left join website_blog_category on website_blog.blog_category_id = website_blog_category.id
        order by website_blog.publish_time desc 
    """)
}


blogs.list = blogs.list || []

blogs.list.map(blog => {
    var tags = db.camel().select(
        """
        select * from website_blog_tag a1 left join website_blog_tag_join a2
        on a1.id = a2.tag_id
        where a2.blog_id = #{blog.id}
        """
    )
    blog.tags = tags
})

return blogs;